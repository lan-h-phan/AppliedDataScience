---
title: "Exercise 1"
author: "Lan Phan & Angelica Leon"
date: "9/25/2022"
output:
  html_document: default
  pdf_document: default
---

## Wickham & Grolemund (2017)
## Chapter 3

3.2.4 Exercises

1. Run ggplot(data = mpg). What do you see?

```{r}
library(tidyverse)
ggplot(data = mpg)
```
This code outputs an empty plot.

2. How many rows are in mpg? How many columns?
```{r}
mpg
nrow(mpg)
ncol(mpg)
```

There are 234 rows and 11 columns. We also know this from the 234 × 11 tibble description. 

3. What does the drv variable describe? Read the help for ?mpg to find out.
```{r}
?mpg
```
The drv variable indicates the type of drive train, where f = front-wheel drive, r = rear wheel drive, and 4 = 4wd.

4. Make a scatterplot of hwy vs cyl.
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = cyl, y = hwy))

```

5. What happens if you make a scatterplot of class vs drv? Why is the plot not useful?
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = drv, y = class))
```

This plot isn't useful because both variables are categorical, where class has 7 categories and drv only has 3 categories represented in this graph. Since there are limited combinations of both variables revealed through this plot, no discernible pattern can be identified.   

3.3.1 Exercises

1. What’s gone wrong with this code? Why are the points not blue?  
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = "blue"))
```

The points are not blue because there is a missing close parentheses after the "aes" function. The "color="blue" code is being read as part of the aes function, rather than part of the mapping function. To correct, the code should read as: 
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), color = "blue")
```

2. Which variables in mpg are categorical? Which variables are continuous? (Hint: type ?mpg to read the documentation for the dataset). How can you see this information when you run mpg?
```{r}
?mpg
mpg
```

The following variables are categorical: manufacturer, model, trans, drv, fl, class. 
The following variables are continuous: displ, year, cyl, cty, hwy. 
We know this based on the classification type under each variable name when mpg is ran. The categorical variables are indicated by <chr> and the continuous variables are indicated by <dbl> or <int>. Reading the descriptions from ?mpg of each variable can also help us make inferences about the type of variable.  

3. Map a continuous variable to color, size, and shape. How do these aesthetics behave differently for categorical vs. continuous variables?
```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy, color = displ))
```

When mapping a cont. variable to color, the scale appears in a gradient  (defaulted to blue), where the colors of the gradient represent values of the cont. variable. 

```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy, size = displ))
```

When mapping a cont. variable to size, we see the values of the variable represented through shape size.  Here, larger values are represented as larger circles and smaller values are represented by smaller circles. 

We get an error message when we try to map a continuous variable to shape. The code was not included so we could generate this html.

4. What happens if you map the same variable to multiple aesthetics?

```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy, color = displ, size = displ))
```

We see the color and size aesthetics combined in the plot; here, the points reflect both the size scale and color gradient corresponding to the values of the cont. variable. 

5. What does the stroke aesthetic do? What shapes does it work with? (Hint: use ?geom_point)

Stroke adjusts the thickness of the border for shapes plotted.  It only works for shapes 21-24. The larger the number, the larger in area of the border is. A sample is below: 

```{r}
?geom_point

ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy), shape = 24,
             fill = 'blue', size = 3, stroke = 5, color = 'red')
```

6. What happens if you map an aesthetic to something other than a variable name, like aes(colour = displ < 5)?  Note, you’ll also need to specify x and y.

```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y = hwy, color = displ < 5))
```

Using aes(colour = displ < 5) categorizes the displ variable dichotomously into "True" and "False" categories, where values <5 fall under "True" and everything else (values > or = 5) falls under "False". These two categories are color-coded with different colors (here, blue = true and red = false).

3.5.1 Exercises
1. What happens if you facet on a continuous variable?

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~cty)
```

When you facet on a continuous variable, it facets according to each level of the variable represented in the dataset. I'd imagine this would not be helpful if there were many levels/values of a continuous variable represented in the data set. Categorical variables tend to have fewer levels, and therefore faceting could be more useful. 

2. What do the empty cells in plot with facet_grid(drv ~ cyl) mean? How do they relate to this plot?
```{r}
ggplot(data = mpg) +
  geom_point(mapping = aes(x = displ, y= hwy)) + 
  facet_grid(drv ~ cyl)
```

The empty grids indicate that there are no observations for that combination of cyl and drv. 

3. What plots does the following code make? What does . do?
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) +
  facet_grid(drv ~ .)

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) +
  facet_grid(. ~ cyl)
```

The first plot plots hwy vs. displ, where the rows are facetted by drv. 
The second plot plots hwy vs. displ, where the columns are facetted by cyl. 

4. Take the first faceted plot in this section. What are the advantages to using faceting instead of the colour aesthetic? What are the disadvantages? How might the balance change if you had a larger dataset?

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 2)
```

A main advantage of using facetting instead of coloring is it allows for more direct interpretation of the plot based on the facetted variable, rather than having to refer back and forth to the legend vs. plot, and having to mentally keep track of what each color represents. It also minimizes the noise/busyness of having many colors on one plot. 

5. Read ?facet_wrap. What does nrow do? What does ncol do? What other options control the layout of the individual panels? Why doesn’t facet_grid() have nrow and ncol arguments?
For the function facet_wrap(), nrow and ncol allow you to indicate the number of rows and columns. facet_grid() does not have nrow and ncol since the number of rows and columns are set by to the number of unique levels in the row/column variables.

6. When using facet_grid() you should usually put the variable with more unique levels in the columns. Why?
When facetting a plot, it is usually easier to make comparisons of plots when they are side by side (i.e., facetted by columns), rather than making comparisons of plots that are above and below each other.  The fact that the DV typically sits on the y-axis contributes to why the former is easier for interpretation than the latter.

3.6.1 Exercises

1. What geom would you use to draw a line chart? A boxplot? A histogram? An area chart?
I'd use the following to draw each respectively: geom_line(), geom_boxplot(), geom_histogram(), and geom_area().

2. Run this code in your head and predict what the output will look like. Then, run the code in R and check your predictions.

I predicted this graph would illustrate hwy vs. displ, where the points are color-coded by the categories of drv. geom_smooth() will draw a different line, with a different linetype, for each unique value of the variable that's mapped, so we should see 3 unique lines representing the levels of drv, each abiding to the color-coding scheme.

```{r}
ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + 
  geom_point() + 
  geom_smooth(se = FALSE)
```

3. What does show.legend = FALSE do? What happens if you remove it? Why do you think I used it earlier in the chapter?

show.legend = FALSE hides the legend on the right hand side of the plot, when a variable is mapped to an aesthetic. show.legend is defaulted to = TRUE. It was used earlier in the chapter to hide the legend of the example plot. 

4. What does the se argument to geom_smooth() do?
```{r}
?geom_smooth
```
The se argument displays the confidence interval around smooth (TRUE by default).

5. Will these two graphs look different? Why/why not?
```{r}

ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point() + 
  geom_smooth()

ggplot() + 
  geom_point(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_smooth(data = mpg, mapping = aes(x = displ, y = hwy))
```

No, these graphs will look identical because they are asking for the same output, just written in different ways. In the first code, the plot is mapped and then "geomed". In the second code, the plots are mapped and defined within each geom. 

6. Recreate the R code necessary to generate the following graphs.

```{r}
ggplot(data = mpg, mapping = aes(y = hwy, x = displ)) + 
  geom_point() +
  geom_smooth(se = FALSE)

ggplot(data = mpg, mapping = aes(y = hwy, x = displ)) + 
  geom_point() +
  geom_smooth(mapping = aes(group = drv), se = FALSE, show.legend = FALSE)

ggplot(data = mpg, mapping = aes(y = hwy, x = displ)) + 
  geom_point(mapping = aes(color = drv)) +
  geom_smooth(mapping = aes(color = drv, group = drv), se = FALSE)

ggplot(data = mpg, mapping = aes(y = hwy, x = displ)) + 
  geom_point(mapping = aes(color = drv)) +
  geom_smooth(se = FALSE)

ggplot(data = mpg, mapping = aes(y = hwy, x = displ)) + 
  geom_point(mapping = aes(color = drv)) +
  geom_smooth(mapping = aes(linetype = drv), se = FALSE)

ggplot(data = mpg, mapping = aes(y = hwy, x = displ)) + 
  geom_point(mapping = aes(fill = drv), color = 'white', stroke = 3, shape = 21, size = 4)
```

3.7.1 Exercises

1.What is the default geom associated with stat_summary()? How could you rewrite the previous plot to use that geom function instead of the stat function?

The default geom associated with stat_summary() is geom_pointrange(). The previous plot can be rewritten with the following code:

```{r}
diamonds %>% group_by(cut) %>% summarize(median_y = median(depth),
                                         min_y = min(depth),
                                         max_y = max(depth)) %>%
  ggplot() +
  geom_pointrange(mapping = aes(x = cut, y = median_y, ymin = min_y, ymax = max_y)) +
  labs(y = 'depth')
```

2. What does geom_col() do? How is it different to geom_bar()?

geom_col() and geom_bar() create bar charts. geom_bar() makes the height of the bar proportional to the number of cases in each group. On the other hand, geom_col() makes the heights of the bars to represent values in the data. The heights of the bars represent the values in the data. geom_bar() uses stat_count() by default and counts the number of cases at each x position, while geom_col() uses stat_identity() and leaves the data as is.

3. Most geoms and stats come in pairs that are almost always used in concert. Read through the documentation and make a list of all the pairs. What do they have in common?

Geom & Stats Pairs: 
geom_bar() &	stat_count()
geom_bin2d() &	stat_bin_2d()
geom_boxplot() &	stat_boxplot()
geom_contour_filled()	& stat_contour_filled()
geom_contour() & stat_contour()
geom_count()	& stat_sum()
geom_density_2d()	& stat_density_2d()
geom_density()	& stat_density()
geom_dotplot()	& stat_bindot()
geom_function()	& stat_function()
geom_sf()	& stat_sf()
geom_sf()	& stat_sf()
geom_smooth()	& stat_smooth()
geom_violin()	& stat_ydensity()
geom_hex()	& stat_bin_hex()
geom_qq_line()	& stat_qq_line()
geom_qq()	& stat_qq()
geom_quantile()	& stat_quantile()
The names of the geoms and stats pairs tend to be the same (e.g., geom_boxplot() and stat_boxplot()). 

4.What variables does stat_smooth() compute? What parameters control its behaviour?

stat_smooth() calculates the following variables: 
y or x : predicted value
ymin or xmin: lower pointwise confidence interval around the mean
ymax or xmax : upper pointwise confidence interval around the mean
se: standard error 

The following parameters control the behavior: 
method
formula
se 
na.rm 
method.args
(see ?stat_smooth() for list under arguments)

5. In our proportion bar chart, we need to set group = 1. Why? In other words what is the problem with these two graphs?

In the proportion bar chart, the bars in the plot will have the same height ( a height of 1), if the group is not set to group = 1. Thus, the problem with the two graphs is that the plots have the same height, since geom_bar() makes the height of the bar proportional to the number of cases in each group. Setting group=1 will correct for this. 

3.8.1 Exercises
1. What is the problem with this plot? How could you improve it?
```{r}
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +
  geom_point()
```

In this plot, many points of hwy and cty overlap each other. This problem is known as overplotting.  You can use a jitter position adjustment to decrease overplotting. position = "jitter" adds a small amount of random noise to each point and therefore spreads the points out more. 
```{r}
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +
  geom_point(position = "jitter")
```

2.What parameters to geom_jitter() control the amount of jittering?

There are two arguments to jitter that control the amount of jittering:
width, which controls the amount of horizontal displacement 
height, which  controls the amount of vertical displacement
The default introduces noise in both directions.

3. Compare and contrast geom_jitter() with geom_count().

geom_jitter() adds a small amount of random noise to each point to reduce overplotting. Thus, the locations of x and y are changed slightly. 
geom_count() does not change the location of x and y. If there are many points on one location, then the size of the points will increase. In other words, values with more observations will be larger than those with fewer observations. 

4.What’s the default position adjustment for geom_boxplot()? Create a visualisation of the mpg dataset that demonstrates it.

The default position adjustment for geom_boxplot() is "dodge2". You can see in the below that there are multiple boxplots for each level fo the DV (drv), and dodge2 positions the boxplots so you can see each and they are not overlapping. 
```{r}
ggplot(data = mpg, aes(x = hwy, y = drv, color = class)) +
  geom_boxplot()

```

3.9.1 Exercises
1.Turn a stacked bar chart into a pie chart using coord_polar().

```{r}

ggplot(data = mpg) + 
  geom_bar(mapping = aes(x = drv, fill = manufacturer))+
  coord_polar()
```

2.What does labs() do? Read the documentation.

labs() lets you edit the axis, plot, and legend labels.

3.What’s the difference between coord_quickmap() and coord_map()?

The coord_quickmap() function projects a portion of the earth, which is approximately spherical, onto a flat 2D plane. This function does not preserve straight lines.  On the other hand, coord_quickmap() is a quick approximation that does preserve straight lines. 

4.What does the plot below tell you about the relationship between city and highway mpg? Why is coord_fixed() important? What does geom_abline() do?

```{r}
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +
  geom_point() + 
  geom_abline() +
  coord_fixed()
```

The plot indicates a positive linear relationship between hwy and cty (as cty increases, hwy increases). geom_abline() adds the diagonal line to the plot, with a slope of 1. This allows for us to gauge the approximate slope of the relationship between hwy and cty, according to this reference line; the slope of the positive linear relationship between hwy and cty is close to 1 (a unit increase in cty is associated with a unit increase in hwy). 

coord_fixed forces a specified ratio between the physical representation of data units on the axes.The ratio represents the number of units on the y-axis equivalent to one unit on the x-axis. The default is ratio = 1 and ensures that one unit on the x-axis is the same length as one unit on the y-axis. Here, it allows us to more accurately reference the line from geom_abline() and infer the slope of 1. 

## Chapter 4

4.4 Exercises
1. Why does this code not work?

The code doesn't work because in the second line, there is a typo. "my_varıable" needs to be corrected to "my_variable" to run seamlessly. See the corrected code below: 
```{r}
my_variable <- 10
my_variable
```

2. Tweak each of the following R commands so that they run correctly:
See corrections below:

```{r}
library(tidyverse)

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))

filter(mpg, cyl == 8)
filter(diamonds, carat > 3)
```

3. Press Alt + Shift + K. What happens? How can you get to the same place using the menus?

The Keyboard Shortcut Quick Reference appears.  Using the menus, click Help, then Keyboard Shortcuts Help.

##Chapter 5

Install data
```{r}
library(nycflights13)
colnames(flights)
```

5.2.4 Exercises

1. Find all flights that
```{r}
#1. Had an arrival delay of two or more hours
filter(flights, arr_delay >= 120)
#2 Flew to Houston (IAH or HOU)
filter(flights, dest == "IAH" | dest == "HOU")
#3 Were operated by United, American, or Delta
filter(flights, carrier %in% c("UA", "AA", "DL"))
#4 Departed in summer (July, August, and September)
filter(flights, month %in% 7:9)
#5 Arrived more than two hours late, but didn’t leave late
filter(flights, arr_delay > 120 & dep_delay <= 0)
#6 Were delayed by at least an hour, but made up over 30 minutes in flight
filter(flights, dep_delay >= 60 & (dep_delay - arr_delay > 30))
#7 Departed between midnight and 6am (inclusive)
filter(flights, dep_time >= 0000 & dep_time <= 0600)
```

2. Another useful dplyr filtering helper is between(). What does it do? Can you use it to simplify the code needed to answer the previous challenges? 

Between indicates the lower and upper cut of the values we are looking for. 
For example, we can say x is between 7 and 9 if 7<x<9 
See below for the simplified code

```{r}
filter(flights, between(month, 7, 9))
```

3. How many flights have a missing dep_time? What other variables are missing? What might these rows represent?

```{r}
summary(flights$dep_time) #8255 missing
# What other variables are missing
filter(flights, is.na(dep_time))
#arrival time is also missing
#What might these rows represent?
#these are flights that did not happen/ have not taken place
```

4. Why is NA ^ 0 not missing? 
Because all number ^ 0 equals 1

Why is NA | TRUE not missing? 
This is a "or" statement, so TRUE can be the value regardless of NA

Why is FALSE & NA not missing? 
This is an "and" statement, anything with FALSE will be false

Can you figure out the general rule? (NA * 0 is a tricky counterexample!)
The general rule is that if a rule applies to all numbers, it will apply to NA
The rules for multiplication by 0 is not applicable to infinite numbers

5.3.1 Exercises

1. How could you use arrange() to sort all missing values to the start? (Hint: use is.na()).
```{r}
arrange(flights, desc(is.na(dep_time)), dep_time)
```

2. Sort flights to find the most delayed flights
```{r}
arrange(flights, desc(dep_delay))
#Find the flights that left earliest
arrange(flights, year, month, day, dep_time)
```

3. Sort flights to find the fastest (highest speed) flights.
```{r}
arrange(flights, desc(distance/air_time))
```

4. Which flights travelled the farthest? Which travelled the shortest?
```{r}
arrange(flights, desc(distance))
arrange(flights, distance)
```

5.4.1

1. Brainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.
```{r}
select(flights, dep_time, dep_delay, arr_time, arr_delay)
select(flights, any_of(c("dep_time", "dep_delay", "arr_time", "arr_delay")))
select(flights, starts_with("dep_"), starts_with("arr_"))
```

2. What happens if you include the name of a variable multiple times in a select() call? Nothing happens and the select() function ignores the duplicate

```{r}
select(flights, dep_time, dep_time)
```

3. What does the any_of() function do? Why might it be helpful in conjunction with this vector?

```{r}
vars <- c("year", "month", "day", "dep_delay", "arr_delay")
select(flights, any_of(vars)) #select all the variables in the vector
```

4. Does the result of running the following code surprise you? How do the select helpers deal with case by default? How can you change that default?

```{r}
select(flights, contains("TIME"))
#This is not surprising to ignore uppercase character and makes it easier to search
select(flights, contains("TIME", ignore.case = FALSE))
```

5.5.2

1. Currently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight.

```{r}
flights %>%
  transmute(
       dep_time,
       hour1 = dep_time %/% 100,
       minute1 = dep_time %% 100,
       dep_time = hour1*60 + minute1,
       sched_dep_time,
       hour2 = sched_dep_time %/% 100,
       minute2 = sched_dep_time %% 100,
       sched_dep_time = hour2*60 + minute2)
```

2. Compare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?

```{r}
flights %>%
  transmute(
    dep_time,
    hour1 = dep_time %/% 100,
    minute1 = dep_time %% 100,
    dep_time = hour1*60 + minute1,
    arr_time,
    hour2 = arr_time %/% 100,
    minute2 = arr_time %% 100,
    arr_time = hour2*60 + minute2,
    air_time,
    diff = arr_time - dep_time)
```

air_time and arr_time - dep_time is different even though they should be the same
This could be a change in time zones or entering a new day
  
3. Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?

```{r}
flights %>%
  transmute(dep_delay,
         dep_time = (dep_time %/% 100 * 60 + dep_time %% 100),
         sched_dep_time = (sched_dep_time %/% 100 * 60 + sched_dep_time %% 100),
         dep_delay_diff = dep_delay - dep_time + sched_dep_time)
#dep_delay should be equals to dep_time - schedule_dep_time; 
# which is what we see with difference being 0
```
  
4. Find the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank().

```{r}
arrange(flights, desc(dep_delay))
flights %>%
  mutate(
    rank = min_rank(desc(dep_delay))
  )
```

5. What does 1:3 + 1:10 return? Why?
1:3 + 1:10
It returns a vector that contains values that were the sum of the two vectors
The equation goes: 1+1, 2+2, 3+3, 1+4, 2+5, 3+6, 1+7, 2+8, 3+9, 1+10
The length is equal to the length of 1:10 (warning message)
  
6. What trigonometric functions does R provide?
We can find out by ?Trig
There are cos(), sin(), tan(), acos(), asin(), atan(x), atan2(y, x), cospi(), sinpi(), tanpi()

5.6.7 Exercises

1. Brainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. Consider the following scenarios:
A flight is 15 minutes early 50% of the time, and 15 minutes late 50% of the time
A flight is always 10 mins late
A fight is 30 minutes early 50% of the time, and 30 minutes late 50% of the time.
99% of the time a flight is on time. 1% of the time it’s 2 hours late.
What is more important: arrival delay or departure delay?

If thinking from passengers' perspectives, arrival delay is more important as it affects their next plans or even the next connecting flights.
This is probability. A flight might be 99% on time, but that 1% long delay can affect people more than frequent 10 minutes.

2. Come up with another approach that will give you the same output as not_cancelled %>% count(dest)

```{r}
flights %>%
  filter(!is.na(dep_delay), !is.na(arr_delay)) %>%
  group_by(dest) %>%
  summarise(count = n())
```

And alternative to not_cancelled %>% count(tailnum, wt = distance) (without using count()).

```{r}
flights %>%
  filter(!is.na(dep_delay), !is.na(arr_delay)) %>%
  group_by(tailnum) %>%
  summarise(count = sum(distance))
```

3. Our definition of cancelled flights (is.na(dep_delay) | is.na(arr_delay) ) is slightly suboptimal. Why? Which is the most important column?

The flights can be redirected or landed at another airport, which might not show up in arr_delay or dep_delay
We can look at the actual airtime of the flight (air_time)
  
4. Look at the number of cancelled flights per day. 

```{r}
flights %>%
  mutate(cancel = is.na(arr_time)) %>%
  group_by(year, month, day) %>%
  summarise(count = n(),
            cancelled = sum(cancel)
            ) %>%
  ggplot() +
  geom_point(aes(x = count, y = cancelled))
#More delays seems to be correlated with more flights

#Is there a pattern? Is the proportion of cancelled flights related to the average delay?
flights %>%
  mutate(cancel = is.na(arr_time)) %>%
  group_by(year, month, day) %>%
  summarise(delay = mean(arr_delay, na.rm = TRUE),
            cancelled = sum(cancel)
  ) %>%
  ggplot() +
  geom_point(aes(x = delay, y = cancelled))
#similarly, there are likely more cancelled flights with higher delay rates
```

5. Which carrier has the worst delays? 

```{r}
flights%>%
  group_by(carrier) %>%
  summarise(delay = mean(arr_delay, na.rm = TRUE)) %>%
  arrange(desc(delay))
#F9 has the worst delay
```

Challenge: can you disentangle the effects of bad airports vs. bad carriers? 
Why/why not? (Hint: think about flights %>% group_by(carrier, dest) %>% summarise(n()))

```{r}
flights %>%
  group_by(carrier, dest) %>%
  summarise(delay = mean(arr_delay, na.rm = TRUE),
            count = n()) %>%
  arrange(desc(delay))
```

6. What does the sort argument to count() do. When might you use it?

The count() argument sort data in the order of observations(n)

5.7.1 Exercises

1. Refer back to the lists of useful mutate and filtering functions. Describe how each operation changes when you combine it with grouping.

The mutate and filtering functions would be apply to an entire variable/ column after grouping, they will be applied to the groups we set.

2. Which plane (tailnum) has the worst on-time record?

```{r}
flights %>%
  group_by(tailnum) %>%
  filter(!is.na(arr_delay)) %>%
  summarise(delay = mean(arr_delay)) %>%
  arrange(desc(delay))
#N844MH has the worst record
```
  
3. What time of day should you fly if you want to avoid delays as much as possible?


```{r}
flights %>%
  group_by(hour) %>%
  filter(!is.na(dep_delay)) %>%
  summarise(delay = mean(dep_delay))
#They should leave early in the morning (5-9 a.m.)
```
  
4. For each destination, compute the total minutes of delay. 

```{r}
flights %>%
  group_by(dest) %>%
  filter(!is.na(dep_delay)) %>%
  summarise(delay = sum(dep_delay))
#For each flight, compute the proportion of the total delay for its destination.
flights %>%
  group_by(dest) %>%
  filter(!is.na(dep_delay)) %>%
  summarise(delay = sum(dep_delay),
            prop = dep_delay/ delay)
```

5. Delays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag(), explore how the delay of a flight is related to the delay of the immediately preceding flight.

```{r}
flights %>%
  group_by(origin) %>%
  filter(!is.na(dep_delay)) %>%
  mutate(lag = lag(dep_delay, default = 0)) %>%
  ggplot() +
  geom_point(aes(x = dep_delay, y = lag))
# the delay of the immediate preceding flight is positively correlated with the delay of the next flight
```

6. Look at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error). 

```{r}
flights %>% 
  filter(!is.na(air_time)) %>%
  select(dest, air_time, carrier, flight, tailnum, distance) %>%
  group_by(dest) %>%
  mutate(speed = distance/ air_time) %>%
  arrange(speed)
```

Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?

```{r}
flights %>% 
  filter(!is.na(air_time)) %>%
  select(dest, air_time, carrier, flight, tailnum, distance) %>%
  group_by(dest) %>%
  mutate(speed = distance/ air_time) %>%
  arrange(desc(speed))
```

7. Find all destinations that are flown by at least two carriers. 

```{r}
flights %>%
  filter(!is.na(air_time)) %>%
  group_by(dest) %>%
  summarise(count = n_distinct(carrier, na.rm = TRUE)) %>%
  filter(count > 1)

#Use that information to rank the carriers.
flights %>%
  filter(!is.na(air_time)) %>%
  group_by(carrier) %>%
  summarise(count = n_distinct(dest, na.rm = TRUE)) %>%
  filter(count > 1) %>%
  arrange(desc(count))
```

8. For each plane, count the number of flights before the first delay of greater than 1 hour.

```{r}
flights %>%
  filter(!is.na(air_time)) %>%
  select(tailnum, dep_delay, year, month, day) %>%
  arrange(year, month, day) %>% 
  group_by(tailnum) %>%
  mutate(delay = cumsum(dep_delay >60)) %>%
  summarise(count = sum(delay))
```

## Chapter 6
6.3 Exercises

1. Go to the RStudio Tips twitter account, https://twitter.com/rstudiotips and find one tip that looks interesting. Practice using it!

You can access R Cheatsheets straight from RStudio, by doing the following: Help in Menu Bar > Cheat Sheets > Browse Cheat Sheets...

Bonus tip: You select a number in RStudio, you can hit  Option+Shift ↑ or  Option+Shift ↓ to increment/decrement the number. We tried it in the Console. Cool! 

2. What other common mistakes will RStudio diagnostics report? Read https://support.rstudio.com/hc/en-us/articles/205753617-Code-Diagnostics to find out.

Some common mistakes the diagnostics report will reveal are: 
- Missing arguments, unmatched arguments, partially matched arguments, and too many arguments
- If variable used has no definition in scope or if variable is defined but not used
- Inappropriate use (or lack thereof) of whitespace 


## Bulut & Dejardins (2019)
## Chapter 4

```{r}
install.packages("data.table")
library(data.table)
```

data.table(DT) has a general form
DT[i, j, by]
where i = on which row; j = what to do; and by = grouped by what

4.2.1 Exercises

Import Data Set
The pisa data set is too large so I will use the region6 data instead

```{r}
pisa <- fread("~/Desktop/Academics/2022 Fall/Applied Data Science/Data Sets/region6.csv", na.strings = "")
class(pisa)
print(object.size(pisa), unit = "GB")
```


4.3.1 Exercises

1. Subset all the Female students (ST004D01T) in Germany

```{r}
pisa[CNTRYID == "Germany" & ST004D01T == "Female"]
```

2. How many students are there in Germany? 

```{r}
pisa[CNTRYID == "Germany" & ST004D01T == "Female", 
     .N]
```

3. The .N function returns the length of a vector/ number of rows. Use chaining with the .N function to answer Exercise 2

```{r}
pisa[CNTRYID == "Germany" & ST004D01T == "Female"
     ][,.N]
```

General Learnings
Using j we can:
- select columns
- summarize variables by performing actions on the variables
- and create new variables. 

```{r}
pisa[, 
     .(CNTRYID)] #.() is a shorthand for list()
```

4.4.1 Exercises

First, create a function to convert dichotomous variable to numeric scoring
x a character vector containing "Yes" and "No" responses

```{r}
bin.to.num <- function(x){
  if(is.na(x)) NA
  else if (x == "Yes") 1L
  else if (x == "No") 0L #L is to make sure variable is treated as interger
}
```

Then use this function to create some variables as well as recoding gender

```{r}
pisa[, `:=` (female = ifelse(ST004D01T == "Female", 1, 0),
             sex = ST004D01T, #the := adds the variables directly to the DT
             
             # At my house we have ... 
             desk = sapply(ST011Q01TA, bin.to.num), 
             own.room = sapply(ST011Q02TA, bin.to.num), 
             quiet.study = sapply(ST011Q03TA, bin.to.num), 
             computer = sapply(ST011Q04TA, bin.to.num), 
             software = sapply(ST011Q05TA, bin.to.num),
             internet = sapply(ST011Q06TA, bin.to.num), 
             lit = sapply(ST011Q07TA, bin.to.num), 
             poetry = sapply(ST011Q08TA, bin.to.num), 
             art = sapply(ST011Q09TA, bin.to.num), 
             book.sch = sapply(ST011Q10TA, bin.to.num), 
             tech.book = sapply(ST011Q11TA, bin.to.num), 
             dict = sapply(ST011Q12TA, bin.to.num), 
             art.book = sapply(ST011Q16NA, bin.to.num))]
```

Then create new variables by combining preexisiting ones

```{r}
pisa[, `:=` 
     (math = rowMeans(pisa[, c(paste0("PV", 1:10, "MATH"))], na.rm = TRUE), 
       reading = rowMeans(pisa[, c(paste0("PV", 1:10, "READ"))], na.rm = TRUE), 
       science = rowMeans(pisa[, c(paste0("PV", 1:10, "SCIE"))], na.rm = TRUE))]
```

1. The computer and software variables that were created above ask a student whether they had a computer in their home that they can use for school work (computer) and whether they had educational software in their home (software). Find the proportion of students in Germany and Uruguay that have a computer in their home or have educational software.

```{r}
pisa[CNTRYID %in% c("Germany", "Uruguay"),
     table(computer, software)]

#Adding labels/ make it look prettier
pisa[CNTRYID %in% c("Germany", "Uruguay"),
     .(computer = factor(ST011Q04TA, levels = c("No", "Yes")),
       software = factor(ST011Q05TA, levels = c("No", "Yes")))
][,
  table(computer, software)
]
```

The total number of students in Germany and Uruguay that have a computer in their home OR have educational software is 101 + 5280 + 4669 = 10050; proportion = 10050/10795 = .93

2. For just female students, find the proportion of students who have their own room (own.room) or a quiet place to study (quiet.study).

```{r}
pisa[sex == "Female",
     table(own.room, quiet.study)]
```

The total number of female students who have their own room OR a quiet place to study is 5828 + 10680 + 51172 = 67680; proportion = 67680/ 72608 = .93

4.5.1 Exercises

1. Calculate the proportion of students who have art in their home (art) and the average age (AGE) of the students by gender.

```{r}
pisa[,
     .(art = mean(art, na.rm = TRUE),
       AGE = mean(AGE, na.rm = TRUE)),
     by = .(sex)]
```

2. Within a by argument you can discretize a variable to create a grouping variable. Perform a median split for age within the by argument and assess whether there are age difference associated with having your own room (own.room) or a desk (desk).

```{r}
pisa[,
     .(own.room = mean(own.room, na.rm = TRUE),
       desk = mean(desk, na.rm = TRUE)),
     by =.(age = (AGE < median(AGE)))
     ]
#Older students (above median age) are more likely to have their own room and desks
```

4.8 Lab 

This afternoon when we discuss supervised learning, we’ll ask you to develop some models to predict the response to the question. Do you expect your child will go into a ?” (PA032Q03TA).

1. Recode this variable so that a "Yes" is 1 and a "No" is a -1. Save the variable as sci_car

```{r}
pisa[, 
     "sci_car" := sapply(PA032Q03TA,
                   function(x){
                     if(is.na(x)) NA
                     else if (x == "Yes") 1L
                     else if (x == "No") -1L
                   })
     ][,
       table(sci_car)]
```

2. Calculate descriptives for this variable by sex and country. Specifically, the proportion of test takers whose parents said “Yes” or 1.

```{r}
pisa[,
     .(sci_car = mean(sci_car, na.rm =TRUE)),
     by = .(sex = sex, country = CNTRYID)]
```

After you’ve done this, spend some time investigating the following variables Label and then do the following using data.table and/or sparklyr:

3. Means and standard deviations (sd) for the variables that you think will be most predictive of sci_car

As this is a response by parents, variables that might be the most predictive of sci_car should be observable to parents: Student expected occupational status (BSMJ), Out of school study time (OUTHOURS), Home Educational Resources (HEDRES), Family Wealth (WEALTH) 

```{r}
pisa[,
     .(ExpectedStatus = mean(BSMJ, na.rm = TRUE),
       OutsideStudy = mean(OUTHOURS, na.rm = TRUE),
       HomeRes = mean(HEDRES, na.rm = TRUE),
       Wealth = mean(WEALTH, na.rm = TRUE))
       ]
```

4. Calculate these same descriptives by groups (by sci_car and by sex)

```{r}
pisa[,
     .(ExpectedStatus = mean(BSMJ, na.rm = TRUE),
       OutsideStudy = mean(OUTHOURS, na.rm = TRUE),
       HomeRes = mean(HEDRES, na.rm = TRUE),
       Wealth = mean(WEALTH, na.rm = TRUE)),
     by = .(sci_car, sex)
]
```

5. Calculate correlations between these variables and sci_car

```{r}
pisa[,
      .(ExpectedStatus = cor(sci_car, BSMJ, use = "na.or.complete"),
        OutsideStudy = cor(sci_car, OUTHOURS, use = "na.or.complete"),
        HomeRes = cor(sci_car, HEDRES, use = "na.or.complete"),
        Wealth = cor(sci_car, WEALTH, use = "na.or.complete"))]
```

6. Create new variables
6a. Discretize the math and reading variables using the OECD means (490 for math and 493) and code them as 1 (at or above the mean) and -1 (below the mean), but do in the data.table way without using the $ operator.

```{r}
pisa[, 
     "mathfac" := sapply(math,
                         function(x){
                           if(is.na(x)) NA
                           else if (x >= 490) 1L
                           else if (x < 490) -1L
                         })]

pisa[, 
     "readfac" := sapply(reading,
                         function(x){
                           if(is.na(x)) NA
                           else if (x >= 493) 1L
                           else if (x < 493) -1L
                         })]
```

6b. Calculate the correlation between these variables and the list of variables above.

```{r}
pisa[,
     .(ExpectedStatusM = cor(mathfac, BSMJ, use = "na.or.complete"),
       OutsideStudyM = cor(mathfac, OUTHOURS, use = "na.or.complete"),
       HomeResM = cor(mathfac, HEDRES, use = "na.or.complete"),
       WealthM = cor(mathfac, WEALTH, use = "na.or.complete"),
       ExpectedStatusR = cor(readfac, BSMJ, use = "na.or.complete"),
       OutsideStudyR = cor(readfac, OUTHOURS, use = "na.or.complete"),
       HomeResR = cor(readfac, HEDRES, use = "na.or.complete"),
       WealthR = cor(readfac, WEALTH, use = "na.or.complete"))]
```

7. Chain together a set of operations. For example, create an intermediate variable that is the average of JOYSCIE and INTBRSCI,and then calculate the mean by country by sci_car through chaining

```{r}
pisa[,
     "lovescience" := ((JOYSCIE + INTBRSCI)/2)
     ][,
       .(sci_car = mean(sci_car, na.rm = TRUE)),
       by = .(CNTRYID)]
```

8. Transform variables, specifically recode MISCED and FISCED from characters to numeric variables.

```{r}
ParentEdu = c('MISCED', 'FISCED')
pisa[,
     (ParentEdu) := lapply(.SD, as.numeric), .SDcols = ParentEdu
     ][,
       .(class(MISCED),
         class(FISCED))]
```


