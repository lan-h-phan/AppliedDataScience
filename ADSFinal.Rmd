---
title: "Applied Data Science Final"
author: "Lan Phan"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)

library(dplyr)
library(tidyverse)
library(ggplot2)
library(readxl)
library(data.table)
library(RColorBrewer)
library(ggcorrplot)
library(plotly)
```

# Tidy Data

A series of steps was taken to compile, clean, and sort the various data sets used in this paper. The below codes reflects the initial process. 

```{r US Census, eval = FALSE}
#US Census Cleaning

#race census 2020
racecensus <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/Resonance/BLM/Racecensus.csv")

#turn statenames to a column
racecensus <- racecensus %>%
  pivot_longer(c('Alabama':'Puerto Rico'), names_to = "STATE", values_to = "count")
#labels into column names
racecensus <- racecensus %>%
  pivot_wider(names_from = `Label (Grouping)`, values_from = count)

racecensus <- racecensus %>%
  rename_at(2, ~ "Total") %>%
  rename_at(4, ~ "White") %>%
  rename_at(5, ~ "Black") %>%
  rename_at(6, ~ "Native") %>%
  rename_at(7, ~ "Asian") %>%
  rename_at(8, ~ "Native Hawaiian/PI") %>%
  rename_at(9, ~ "Other") %>%
  rename_at(10, ~ "Inter-racial")

racecensus <- racecensus %>%
  select(1:11)

#save this smaller data set
write_csv(racecensus, "racecensus2020.csv")

#general number count
racecensus <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/Resonance/BLM/racecensus2020.csv")

censusraw <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/Resonance/BLM/Census20202021.csv")

censusraw <- censusraw %>%
  rename_at(7, ~ "PopEstimate") %>%
  select(1:7)

censusclean <- censusraw %>%
  inner_join(racecensus,
            by = c("NAME" = "STATE"))

write_csv(censusclean, "censusclean2020.csv")

```

```{r Police Violence, eval = FALSE}
#Police Violence

statetranslation <- read_xlsx("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/StateTranslation.xlsx")

policeviolence <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/Resonance/BLM/Mapping Police Violence-Grid view.csv")

policeviolence <- policeviolence %>%
  select(age, gender, race, date, city, state, county,
         agency_responsible, cause_of_death,
         circumstances, officer_charged, signs_of_mental_illness,
         allegedly_armed, wapo_armed,
         wapo_flee, wapo_body_camera,
         encounter_type, initial_reason,
         call_for_service,
         pop_white_census_tract, pop_black_census_tract,
         pop_hispanic_census_tract) %>%
  separate(date, into = c("month", "day", "year"), sep = "/") %>%
  left_join(statetranslation,
            by = c("state" = "Abbrev")) 

write_csv(policeviolence, "policeviolence2018-2021.csv")
```

```{r Tweet Count, eval = FALSE}
#Tweet Count Cleaning

tweetcount <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/Resonance/BLM/tweet_counts_per_day.csv")

#just tweet counts from 2018-2021
tweetcount <- tweetcount %>%
  separate(date_str, into = c("year", "month", "date"), sep = "-") %>%
  filter(year %in% c("2018", "2019", "2020", "2021"))

write_csv(tweetcount, "blmtweetcount2018-2021.csv")
```

```{r Demonstration & Bridging Org, eval = FALSE}
# Demonstrations Count Cleaning
#Probably should sort into for BlackLivesMatter; against BLM (i.e. AllLivesMatter); anti and pro-police/ BlueLivesMatter

# Thank you to this person who cleaned this up
# https://github.com/nonviolent-action-lab/crowd-counting-consortium

c3compiled <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/Resonance/BLM/ccc_compiled.csv")

blmkeywords <- c("policing", "racism")

blmkeywords <- str_c(blmkeywords, collapse = "|")


c3compiled <- c3compiled %>%
  separate(date, into = c("year", "month", "date"), sep = "-") %>%
  filter(year %in% c("2018", "2019", "2020", "2021", "2022")) %>%
  filter(grepl("policing|racism", issues, ignore.case = TRUE))

write_csv(c3compiled, "c3clean.csv")

# Bridge building organizations list
bdidat <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/Resonance/BLM/bridging-organizations-data-2022-11-21.csv")
#probably not going to filter this 
#addressing one aspects might have a ripple effect on others
#cooperative vs competitive (demonstrations are more focused on race)
```

## Import Datasets

As some datasets used full state names while others only had abbreviations, I made an excel spreadsheet called statetranslation in order to connect different datasets based on states. The statetranslation document only had 2 variables, one for full state names, the other is the abbreviation. 

```{r}
statetranslation <- read_xlsx("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/StateTranslation.xlsx")

#US Census Data 2020
censusdat <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/censusclean2020.csv")
censusdat <- censusdat %>%
  mutate(rWhite = White/ PopEstimate,
         rBlack = Black/ PopEstimate,
         rAsian = Asian/ PopEstimate,
         rNative = Native/ PopEstimate,
         rOther = Other/ PopEstimate,
         rMixed = `Inter-racial`/ PopEstimate)

#police violence data
policeviolence <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/policeviolence2018-2021.csv")


#tweet count related to BLM 
tweetcount <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/blmtweetcount2018-2021.csv")

#demonstrations count 2018
c3compiled <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/c3clean.csv")

c3allstate <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/c3clean.csv")
c3allstate <- c3allstate %>%
  mutate(type = if_else(valence %in% c(0,1), 1, 2)) %>% #code if the protest is for or against BLM
  count(state, year, type) %>%
  left_join(statetranslation,
            by = c("state" = "Abbrev")) %>%
  filter(!is.na(state))

c3allstate$Name <- as.factor(c3allstate$Name)

c3allstate <- c3allstate %>%
  filter(year %in% c(2018,2019,2020,2021,2022)) %>%
  pivot_wider(names_from = type, values_from = n)

c3allstate$`2`[is.na(c3allstate$`2`)] <- 1


#maybe this website to add tweet counts: https://www.statology.org/sum-specific-rows-in-r/

```

# Data Visualization

## Descriptive Stats
Also here to map tweets over time change: https://www.statology.org/sum-specific-rows-in-r/
## Heatmap

```{r}
#packages
library(ComplexHeatmap)
library(hopach) 
library(circlize)
```

```{r}

#function uncentered correlation and average linkage
uncenter.dist<-function(m) {
  as.dist(as.matrix(distancematrix(m, d="cosangle")))
}


best_col = colorRamp2(c(-2, 0, 2), c("#82A3FF", "grey", "#8B2E2E"))


#Setting up= for the 2018 heatmap


pvsum2018 <- policeviolence %>%
  filter(year == 2018) %>%
  group_by(state) %>%
  summarize(PoliceViolence = n())


datsum2018 <- censusdat %>%
  left_join(statetranslation,
             by = c("NAME" = "Name"))

datsum2018 <- datsum2018 %>%
  left_join(pvsum2018,
            by = c("Abbrev" = "state")) %>%
  mutate(
         rPoliceViolence = (PoliceViolence/ PopEstimate)*100) %>%
  dplyr::select(NAME,
         rWhite, rBlack, rNative, rAsian, rOther, rMixed,
         rPoliceViolence)

hm2018dat<-datsum2018[, c(2:8)]

#name the heatmap rows to their states
rownames(hm2018dat) <- datsum2018$NAME

#z-scoring the data
hm2018dat.scale<-scale(hm2018dat)

#uncentered correlation and average linkage

row.clus18<-hclust(uncenter.dist(hm2018dat.scale), method = "ave")
col.clus18<-hclust(uncenter.dist(t(hm2018dat.scale)), method = "ave")

ht2018 = Heatmap(hm2018dat.scale, name = "2018 Heatmap", 
        cluster_rows=row.clus18, cluster_columns=col.clus18, 
        na_col = "white", col = best_col,
        row_names_gp = gpar(fontsize = 5))

```

```{r}
#2019 Heatmap

#Setting up for the 2019 heatmap


pvsum2019 <- policeviolence %>%
  filter(year == 2019) %>%
  group_by(state) %>%
  summarize(PoliceViolence = n())


datsum2019 <- censusdat %>%
  left_join(statetranslation,
             by = c("NAME" = "Name"))

datsum2019 <- datsum2019 %>%
  left_join(pvsum2019,
            by = c("Abbrev" = "state")) %>%
  mutate(
         rPoliceViolence = (PoliceViolence/ PopEstimate)*100) %>%
  dplyr::select(NAME,
         rWhite, rBlack, rNative, rAsian, rOther, rMixed,
         rPoliceViolence)

hm2019dat<-datsum2019[, c(2:8)]

#name the heatmap rows to their states
rownames(hm2019dat) <- datsum2019$NAME

#z-scoring the data
hm2019dat.scale<-scale(hm2019dat)

#uncentered correlation and average linkage

row.clus19<-hclust(uncenter.dist(hm2019dat.scale), method = "ave")
col.clus19<-hclust(uncenter.dist(t(hm2019dat.scale)), method = "ave")

ht2019 = Heatmap(hm2019dat.scale, name = "2019 Heatmap", 
        cluster_rows=row.clus19, cluster_columns=col.clus19, 
        na_col = "white", col = best_col,
        row_names_gp = gpar(fontsize = 5))
```

```{r}
#2020 Heatmap


pvsum2020 <- policeviolence %>%
  filter(year == 2020) %>%
  group_by(state) %>%
  summarize(PoliceViolence = n())


datsum2020 <- censusdat %>%
  left_join(statetranslation,
             by = c("NAME" = "Name"))

datsum2020 <- datsum2020 %>%
  left_join(pvsum2020,
            by = c("Abbrev" = "state")) %>%
  mutate(
         rPoliceViolence = (PoliceViolence/ PopEstimate)*100) %>%
  dplyr::select(NAME,
         rWhite, rBlack, rNative, rAsian, rOther, rMixed,
         rPoliceViolence)

hm2020dat<-datsum2020[, c(2:8)]

#name the heatmap rows to their states
rownames(hm2020dat) <- datsum2020$NAME

#z-scoring the data
hm2020dat.scale<-scale(hm2020dat)

#uncentered correlation and average linkage

row.clus20<-hclust(uncenter.dist(hm2020dat.scale), method = "ave")
col.clus20<-hclust(uncenter.dist(t(hm2020dat.scale)), method = "ave")

ht2020 = Heatmap(hm2020dat.scale, name = "2020 Heatmap", 
        cluster_rows=row.clus20, cluster_columns=col.clus20, 
        na_col = "white", col = best_col,
        row_names_gp = gpar(fontsize = 5))
```

```{r}

ht2018
ht2019 + ht2020
```

## Heatmap Maps! 

(officially known as Choropleth)

Parts of the codes are from: https://rdvark.net/2021/12/29/pretty-choropleth-maps-with-sf-and-ggplot2/
Maybe do this for bridge-building orgs
```{r}
library(leaflet)
library(sf)

#importing the shape of the U.S.
usmap <- read_sf("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/gz_2010_us_040_00_5m.json")

#Bridge-building Organizations
bdidat <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/Resonance/BLM/bridging-organizations-data-2022-11-21.csv")



choroplot <- bdidat %>%
  group_by(state) %>%
  summarize(count = n()) %>%
  left_join(statetranslation, by = c("state" = "Name")) %>%
  inner_join(censusdat,
             by = c("state" = "NAME")) %>%
  mutate(ratio = (count/PopEstimate)*100) %>%
  inner_join(usmap,
             by = c("state" = "NAME"))

#turn back into a sf object for geom_sf to map out data
choroplot <- st_as_sf(choroplot)


#### Just by count
ggchoroplot <- choroplot %>% 
  ggplot(aes(fill = count)) + # create a ggplot object and 
  # change its fill colour according to median_age
  geom_sf(colour = NA) +# plot all local authority geometries 
  scale_fill_gradient(low = "#82A3FF", 
                      high = "#8B2E2E", na.value = "white") +
  coord_sf(xlim = c(-180, -60), lims_method = "box") + #just zoom in the U.S.
  theme_void() #remove background

ggchoroplot #looks nicer than when with plotly

#also work with plotly
ggplotly(ggchoroplot)
#finally omg
#so beautiful imma cry

```

## Changes in Demonstrations by Year and States

Code adapted from Georgios Karamanis from Tidy Tuesday Challenge https://github.com/gkaramanis/tidytuesday/blob/master/2021/2021-week26/animal-rescues.R
```{r}

###Font Settings
library(showtext)
library(Hmisc)
library(geofacet)
library(ggh4x)


font_add_google("Fira Sans", family = "firasans")
font_add_google("Lato", family = "lato")


stategeofacet <- c3allstate %>%
  ggplot(aes(x = year, group = 1)) +
  geom_line(aes(y = `2`, color = "counter-protest")) +
  geom_line(aes(y = `1`, color = "for BLM")) +
  stat_difference(aes(ymin = `1`, ymax = `2`), alpha = .3) +
  facet_geo(vars(state), grid = "us_state_grid1")

stategeofacet <- stategeofacet +
  # Colors for the lines
  scale_color_manual(values = c("#C32E5A", "#3D85F7")) +
  scale_fill_manual(
    values = c(
      colorspace::lighten("#C32E5A"), 
      colorspace::lighten("#3D85F7"),
      "grey60"),
    labels = c("more BLM", "more counter-protest")
  ) +
  # Add labels
  labs(
    title = "Number of demonstrations \nfor and against #BlackLivesMatter \nfrom 2018-2021",
    caption = "Source: CrowdCountingConsortium"
  ) +
  # Specify the order for the guides
  guides(
    # Order indicates the order of each legend among multiple guides.
    # The guide for 'color' will be placed before the onde for 'fill'
    color = guide_legend(order = 1), 
    fill = guide_legend(order = 2)
  ) 



stategeofacet <- stategeofacet +
  # A minimalistic theme with no background annotations
  theme_minimal() +
  theme(
    # Top-right position
    legend.pos = c(0.875, 0.2),
    # Elements within a guide are placed one next to the other in the same row
    legend.direction = "horizontal",
    # Different guides are stacked vertically
    legend.box = "vertical",
    # No legend title
    legend.title = element_blank(),
    # Light background color
    plot.background = element_rect(fill = "#F5F4EF", color = NA),
    plot.margin = margin(20, 30, 20, 30),
    # Customize the title. Note the new font family and its larger size.
    plot.title = element_text(
      margin = margin(0, 0, -50, 0), 
      size = 14, 
      face = "bold", 
      vjust = 0, 
      color = "grey25"
    ),
    plot.caption = element_text(size = 11),
    # Remove titles for x and y axes.
    axis.title = element_blank(),
    # Specify color for the tick labels along both axes 
    axis.text = element_text(color = "grey40", size = 6),
    # Specify face and color for the text on top of each panel/facet
    strip.text = element_text(face = "bold", color = "grey20")
  )

stategeofacet
```


# Data Analysis

## Topic Modeling 

Because of historical effects, per Dr. Bowers' recommendations, I will group 2018-2019 data together and compare them against 2020-2021 data. 

The codes are from 

Bowers, A.J., Chen, J.(2015) Ask and Ye Shall Receive? Automated Text Mining of Michigan Capital Facility Finance Bond Election Proposals to Identify which Topics are Associated with Bond Passage and Voter Turnout. Journal of Education Finance, 41(2), p.164-196. http://dx.doi.org/10.7916/D8FJ2GDH

```{r topic modeling packages}
library(topicmodels)
library(tm)
library(tidytext)
library(tidyr)
library(stringr)
library(slam)
library(ggrepel)
library(MASS)
library(textstem)
library(readtext)
#package to change the time into something that makes sense
library(anytime)
```

### 2018-2019 Reddit Posts
```{r, eval=FALSE}
#Importing Reddit Data for 2018
comments2018 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2018comments.csv", na.string = "")


posts2018 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2018posts.csv", na.string = "")


#reshaping 2018 data by matching post and comments together based on the post_id 
all2018 <- posts2018[comments2018, on = c(id = "submission")] #right_joining 

all2018$time_created <- anydate(all2018$time_created)

#collapsing all comments to join the posts

collapsed18 <- all2018 %>%
  group_by(id) %>%
  summarize(allcomment = paste(comment_body, collapse = " "))

collapsed18 <- collapsed18 %>%
  left_join(posts2018, by = c("id" = "id")) %>%
  filter(!is.na(comms_num)) 

collapsed18$alltext = paste(collapsed18$title, collapsed18$allcomment)

collapsed18$time_created <- anydate(collapsed18$time_created)

#### 2019

posts2019 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2019posts.csv", na.string = "")

comments2019 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2019comments.csv", na.string = "")



#reshaping 2018 data by matching post and comments together based on the post_id 
all2019 <- posts2019[comments2019, on = c(id = "submission")] #right_joining 

all2019$time_created <- anydate(all2019$time_created)

#collapsing all comments to join the posts

collapsed19 <- all2019 %>%
  group_by(id) %>%
  summarize(allcomment = paste(comment_body, collapse = " "))

collapsed19 <- collapsed19 %>%
  left_join(posts2019, by = c("id" = "id")) %>%
  filter(!is.na(comms_num)) 

collapsed19$alltext = paste(collapsed19$title, collapsed19$allcomment)

collapsed19$time_created <- anydate(collapsed19$time_created)

reddit1819 <- rbind(collapsed18, collapsed19)

reddit1819all <- rbind(all2018, all2019)
#fwrite(reddit1819all, "reddit1819all.csv")
#posts1819 <- rbind(posts2018, posts2019)
#posts1819$time_created <- anydate(posts1819$time_created)
```

```{r tidy text 18-19, eval = FALSE}

# I did this step in python to clean out names, organization, and location manually
# using the spaCy package

posts1819 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/clean1819all.csv", na.string = "")


#random select 100 rows cause it is too much 
#ctmdat <- sample_n(discussion2018p, 200)
  
```


*Learning Notes from Dr. Bowers*
topicmodels and tm needs a very specific type of data, called a DocumentTermMatrix. We’ll call specific corpus and DocumentTermMatrix functions here to create the data matrix required for the topic model.

Note here that this is where we stem, stopword, set the min word length, remove numbers, and remove punctuation.

So this sets up our data matrix DTM. We then need to know how to set “alpha” the hyperparameter, so we need some information on the tfidf (term frequency-inverse document frequency)

```{r, eval = FALSE}
#Build CMT
corpus <- Corpus(VectorSource(posts1819$alltext))

#clean using the codes from Wang et al., 2016
REPLACE_STRING <- content_transformer(function(x, pattern, y)
gsub(pattern, y, x, perl = TRUE))

corpus <- tm_map(corpus, REPLACE_STRING, "\\s*â€”\\s*", "") #remove hyphens
corpus <- tm_map(corpus, REPLACE_STRING, "[[:punct:]]", "") #remove punctuations
corpus <- tm_map(corpus, stemDocument) #stemming

corpus <- tm_map(corpus, removeWords, c("cmv", "http", "www", "wiki", "reddit", "com", "police", "officer", "officers", "lives", "matter", "blm", "d't", "n't", "people", "trump", "american", "cops", "americans", "blacklivesmatter", "russian"))


#text manipulation
text_DTM <- DocumentTermMatrix(corpus, control = list(stemming = TRUE, 
                                                      stopwords = TRUE, minWordLength = 3, 
                                                      removeNumbers = TRUE, removePunctuation = TRUE))

term_tfidf <- tapply(text_DTM$v / row_sums(text_DTM)[text_DTM$i], text_DTM$j, mean) * log2(nDocs(text_DTM)/col_sums(text_DTM > 0))

summary(term_tfidf)

plot(density(term_tfidf))
                            
```
Rule of thumb is to use the median from the graph for alpha. Here it's around 1.05
We will cut down the total number of terms by setting alpha so that we don't have too many common terms no unique ones. 
```{r, eval = FALSE}
alpha <- 0.003 #is the median
text_DTM_trimmed <- text_DTM[row_sums(text_DTM) >0, term_tfidf >= alpha]
dim(text_DTM_trimmed)
```

**10-fold Cross Validation**

We set up the 10 fold cross validation, selecting 10% of the data for each of the 10 folds

```{r, eval = FALSE}
# Cross validation

control_CTM_VEM <- list(
  estimate.beta = TRUE, verbose = 0, prefix = tempfile(), save = 0, keep = 0,
  seed = as.integer(Sys.time()), nstart=1L, best = TRUE,
  var = list(iter.max=100, tol=10^-6),
  em = list(iter.max=500, tol=10^-4),
  cg = list(iter.max=100, tol=10^5)
)

# use 10-fold CV to determine k!
# randomly divide the data into 10 folds.

set.seed(123)
topics <- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15) ##set k equals to 2 3 4 5 6 7 8 9 10 15.
seed <- 2
D <- length(posts1819$alltext) 
folding <- sample(rep(seq_len(10), ceiling(D))[seq_len(D)])
table(folding)
```

**Learning Notes**
We train and test using the 10-fold cross validation that we just set up. Then plot by perplexity to see where the elbow is to select the correct number of topics k

Note: I waited for 12 hours (overnight) and it was still not done processing, so I decided to just do it in Python, which took a few minutes. The python code and output are attached in a separate pdf document. However, I will still present the R code as it would have been here. 

```{r, eval=FALSE}

#the code originally did not work because there were columns that had all 0 responses
#so we had to remove them
raw.sum = apply(text_DTM_trimmed, 1, FUN = sum)
text_DTM_trimmed = text_DTM_trimmed[raw.sum!= 0,]


## write a loop to automatically output the perplexity
perp_by_col <- vector()
for (k in topics) {
  perp_by_row <- vector()
  for (chain in seq_len(10)) {
    training <- CTM(text_DTM_trimmed[folding != chain,], k = k,
                    control = control_CTM_VEM)
    testing <- CTM(text_DTM_trimmed[folding == chain,], model = training,
                   control = control_CTM_VEM)
    perp_by_row <- rbind(perp_by_row, perplexity(testing))
  }
  perp_by_col <- cbind(perp_by_col, perp_by_row)
}

 #wow this is indeed taking forever
```

```{r, eval = FALSE}
# Plot perplexity
transpose <- t(perp_by_col)
matplot(transpose, type = "l", col = rainbow(9), lty = 2, lwd = 2, ylab = "Perplexity", xlab = "K", main = "CTM-10-fold cross validation", xaxt="n")
axis(1, at=1:10, labels = c("k=2", "k=3", "k=4", "k=5", "k=6", "k=7", "k=8", "k=9", "k=10", "k=15"), cex=0.5)

perp_by_col_mean <- colMeans(perp_by_col)

lines(perp_by_col_mean, col = "black", lwd = 4, lty = 1)
led <- c("fold=2", "fold=3", "fold=4", "fold=5", "fold=6", "fold=7", "fold=8", "fold=9", "fold=10", "Average")
legend("topright", led, lwd = 2, lty = 2, col = c(rainbow(9), 'black'), cex = 0.65)

abline(v = 4, col = "gray60", lty = 2)
```

```{r, eval = FALSE}
{plot(perp_by_col_mean, pch = 20, ylab = 'Perplexity', xlab = "K", main = "CTM-10-fold cross validation", 
      xaxt = "n") 
  axis(1, at = 1:10, labels = c("k=2","k=3","k=4","k=5","k=6","k=7","k=8","k=9","k=10","k=15"), cex = 0.5)
  lines(perp_by_col_mean, lwd = 1, lty = 2, col = "red")}
```
Alright this is  10 topics

*Correlated Topic Model*
Running 10 correlated topic model
```{r, eval = FALSE}
control_CTM_VEM1 <- list(
  estimate.beta = TRUE, verbose=0, prefix=tempfile(),save=0,keep=0,
  seed=1421313709,nstart=1L,best=TRUE,
  var=list(iter.max=500,tol=10^-6),
  em=list(iter.max=1000,tol=10^-4),
  cg=list(iter.max=500,tol=10^5)
)

################## K = 10 ##################

CTM3 <- CTM(text_DTM_trimmed, k = 10, control = control_CTM_VEM1)
CTM3
```

*Examining the Topic Output*
```{r, eval = FALSE}

## Table the documents by topics
main_topic_table3 <- as.data.frame(table(topics(CTM3)))
colnames(main_topic_table3) <- c("Topic", "Frequency")
print(main_topic_table3)


#what are the top 10 words for the 3 topics
terms(CTM3,10)
```
```{r, eval = FALSE}
# Using this: https://www.tidytextmining.com/topicmodeling.html
# Use tidyverse to look at the CTM results a bit more more

tidy_topics <- tidy(CTM3, matrix = "beta")

top_terms <- tidy_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta) 

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_fill_brewer(palette = "Paired") +
  scale_y_reordered() +
  theme_bw()
```
```{r, eval = FALSE}

## Topics
topics3 <- posterior(CTM3)$topics
## Let's look at the probability of each document info fits into each of the topics
topics3 <- as.data.frame(topics3)
nrow(topics3)
rownames(topics3) <- posts1819$name
print(topics3)
## Let's look at which topic each document is assigned to one of the topics.
main_topic3 <- as.data.frame(topics(CTM3))
rownames(main_topic3) <- posts1819$doc_id
colnames(main_topic3) <- "Main_Topic"
print(main_topic3)
```

### 2020-2021 Reddit Posts
The process is similar to the 2018-2019 posts. The actual analyses in Python is attached in the separate pdf document. 

## Sentiment Analysis
Sentiment analysis codes were adapted from 
Silge, J., Robinson, D. (2017). Text Mining with R: A Tidy Approach. O'Reilly.
```{r packages for sentiment}
library(tidytext)
library(textdata)
nrc <- get_sentiments("nrc") 
```

### Sentiments across all data

#### 2018-2019 Reddit Posts
```{r data 18-19 import}
#this is the dataset with topics already assigned to them from python
reddit1819 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit1819tm.csv", na.string = "")
reddit1819$time_created <- anydate(reddit1819$time_created)
colnames(reddit1819)
reddit1819 <- reddit1819 %>%
  dplyr::select(id, allcomment, time_created, title, post_flair, score, upvote_ratio, comms_num, created, body, author, author_flair, subreddit, alltext, topictext, Topic)
```
```{r tidy 18-19 w sentiment}
tidy_r1819 <- reddit1819 %>%
  unnest_tokens(word, topictext)

tidy_r1819 %>%
  inner_join(nrc) %>%
  group_by(sentiment, Topic) %>%
  count(sort = TRUE)
```
```{r sentiment afinn plot}

tidy_r1819 %>%
  inner_join(get_sentiments("afinn")) %>%
  separate(time_created, into = c("year", "month", "date"), sep = "-", remove = FALSE) %>%
  unite(ym, year, month, sep = "-") %>%
  arrange(ym, .by_group = TRUE) %>%
  summarise(sentiment = sum(value),
            Topic = Topic,
            index = ym) %>%
  ggplot(aes(index, sentiment, fill = Topic)) +
  geom_col(show.legend = FALSE) +
  xlab("")+
  facet_wrap(~Topic, ncol = 3) +
  theme_bw()
```
```{r}
tidy_r1819 %>%
  inner_join(get_sentiments("bing")) %>%
  separate(time_created, into = c("year", "month", "date"), sep = "-", remove = FALSE) %>%
  unite(ym, year, month, sep = "-") %>%
  arrange(ym, .by_group = TRUE) %>%
  count(Topic, index = ym, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative) %>%
  ggplot(aes(index, sentiment, fill = Topic)) +
  geom_col(show.legend = FALSE) +
  xlab("")+
  facet_wrap(~Topic, ncol = 3) +
  theme_bw()
```
Shows similar direction to afinn, but afinn has more variance, so I will keep afinn package
#### 2020-2021 Reddit Posts
```{r data 20-21 import}
#this is the dataset with topics already assigned to them from python with cleaned data
reddit2021 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2021tm.csv", na.string = "")
reddit2021$time_created <- anydate(reddit2021$time_created)
colnames(reddit2021)
reddit2021 <- reddit2021 %>%
  dplyr::select(id, allcomment, time_created, title, post_flair, score, upvote_ratio, comms_num, created, body, author, author_flair, subreddit, alltext, topictext, Topic)
```

```{r tidy 20-21 data}
tidy_r2021 <- reddit2021 %>%
  unnest_tokens(word, topictext)
```
```{r}
tidy_r2021 %>%
  inner_join(nrc) %>%
  group_by(sentiment, Topic) %>%
  count(sort = TRUE)
```

```{r sentiment plot}

tidy_r2021 %>%
  inner_join(get_sentiments("afinn")) %>%
  separate(time_created, into = c("year", "month", "date"), sep = "-", remove = FALSE) %>%
  unite(ym, year, month, sep = "-") %>%
  arrange(ym, .by_group = TRUE) %>%
  summarise(sentiment = sum(value),
            Topic = Topic,
            index = ym) %>%
  ggplot(aes(index, sentiment, fill = Topic)) +
  geom_col(show.legend = FALSE) +
  xlab("")+
  facet_wrap(~Topic, ncol = 3) +
  theme_bw()
```


What if we find common topic across both data and see how they change over time
### Sentiment and Engagement
What if we study the posts with higher engagement vs. lower engagement 
Run correlations?
```{r}
nrc_small <- get_sentiments("nrc") %>%
  filter(sentiment %in% c("trust", "anticipation", "joy", 
                        "fear", "anger", "disgust", "sadness"))

#just post titles
res2021 <- reddit2021 %>%
  unnest_tokens(word, title) %>%
  inner_join(nrc_small) %>%
  group_by(sentiment, id)# %>%
  #mutate(count = n())

setDT(res2021)
#make the frame longer so that each sentiment is a column 
res2021 <- dcast.data.table(res2021, id~sentiment, value.var= 'sentiment')

res2021 <- res2021 %>%
  inner_join(reddit2021, by = c("id" = "id"))
#rm(reddit2021) #for now as vector memory exhausted
colnames(res2021)
res2021corr <- res2021 %>%
  dplyr::select(comms_num, score, upvote_ratio, joy, trust, anticipation, anger, disgust, fear, sadness)

correlation_matrix <- round(cor(res2021corr),1)
corrp.mat <- cor_pmat(res2021corr) #calculating p-vals

ggcorrplot(correlation_matrix, hc.order =TRUE, 
           type ="upper", p.mat = corrp.mat, insig="blank") 

```
Fear and trust sentiments in the title are positively correlated with number of comments and upvote ratio

Disgust and sadness are negatively correlated with upvote_ratio
### Zooming in: Change My View in 2020-2021
What about comments? 

```{r}
#all CMV posts
posts2018 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2018posts.csv", na.string = "")
posts2019 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2019posts.csv", na.string = "")
posts2020 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2020posts.csv", na.string = "")
posts2021 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2021posts.csv", na.string = "")
cmvAllp <- rbind(posts2018, posts2019, posts2020, posts2021, fill = TRUE)
cmvAllp <- cmvAllp[subreddit == "changemyview",]

#all CMV comments
comments2018 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2018comments.csv", na.string = "")
comments2019 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2019comments.csv", na.string = "")
comments2020 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2020comments.csv", na.string = "")
comments2021 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2021comments.csv", na.string = "")
redditAllc <- rbind(comments2018, comments2019, comments2020, comments2021, fill = TRUE)



#just take comments from posts with more than 2 comments
cmvAlldat <- cmvAllp %>%
  left_join(redditAllc, by = c("id" = "submission"))
#cmvAlldat <- cmvAlldat[!is.na(comment_body),]

colnames(cmvAlldat)
cmvAlldat <- cmvAlldat %>%
  dplyr::select(time_created, title, post_flair, score, upvote_ratio, id, comms_num, author, author_flair, subreddit, comment_id, comment_parent_id, comment_body, comment_score, comment_author, comment_issubmitter)

cmvAlldat$time_created <- anydate(cmvAlldat$time_created)

```
Using code from Sanchez, G. (2021) Handling Strings With R http://www.gastonsanchez.com/r4strings

Summary stats
```{r}
summary(cmvAlldat)
```
```{r tidy data}
cmvAlldat$title <- tolower(cmvAlldat$title)
cmvAlldat$comment_body <- tolower(cmvAlldat$comment_body)
#cmvAlldat$comment_body <- lemmatize_strings(cmvAlldat$comment_body)
#taking forever

cmvAllcomm <- cmvAlldat %>%
  unnest_tokens(word, comment_body) %>%
  anti_join(stop_words)

```
Lexical Diversity from Quanteda package: https://tutorials.quanteda.io/statistical-analysis/lexdiv/
Lexical diversity measures number of unique types of tokens and the length of a document. It is useful for analysing speakers' or writers' linguistic skills, or the complexity of ideas expressed in documents
Also codes from: https://towardsdatascience.com/lyrics-analysis-5e1990070a4b
```{r}
library(quanteda)
library(quanteda.textstats)


corpus_cmv <- corpus(cmvAlldat, text_field = "comment_body")
#get number of tokens in each comments
docvars(corpus_cmv)$ntoken <- corpus_cmv %>%
  ntoken()
#filter out shorter comments
corpus_cmv <- corpus_subset(corpus_cmv, ntoken > 10)

  
toks_cmv <- tokens(corpus_cmv)

dfmat_cmv <- dfm(toks_cmv, remove = stopwords("en"))

tstat_lexdiv <- textstat_lexdiv(dfmat_cmv, remove_numbers = TRUE, remove_hyphens = TRUE)

#merge document back
lexdiv_cmv <- bind_cols(docvars(corpus_cmv), tstat_lexdiv[,1:2])
tail(lexdiv_cmv, 5)
```

```{r}
#Study lexical diversity for people based on deltas

lexdiv_df <- lexdiv_cmv %>%
  group_by(author_flair, author) %>%
  summarize(count = n(),
            postcomm = mean(comms_num),
            score = mean(comment_score),
            token_num = mean(ntoken),
            lexdiv = mean(TTR)) %>%
  mutate(delta = gsub("∆", "", author_flair))

lexdiv_df <- lexdiv_df %>%
  group_by(delta) %>%
  summarize(redditors = n(),
            post_num = mean(count),
            comms_num = mean(postcomm),
            comm_score = mean(score),
            ntoken = mean(token_num),
            rlexdiv = mean(lexdiv))


lexdiv_df <- lexdiv_df[-c(40:43),]

  

lexdiv_df$delta <- as.numeric(lexdiv_df$delta)

lexdiv_df[36,]$delta <- 7 #for some reason this row has to be assigned
#lexdiv_df[40,]$delta <- 0


#graph
correlation_matrix <- round(cor(lexdiv_df),1)
corrp.mat <- cor_pmat(lexdiv_df) #calculating p-vals

ggcorrplot(correlation_matrix, hc.order =TRUE, 
           type ="upper", p.mat = corrp.mat, insig="blank") 


```

Adding on sentiments
```{r}
#afinn <- get_sentiments("afinn") 
 # summarise(sentiment = sum(value),

cmvAllcomm <- cmvAllcomm %>%
  inner_join(nrc_small) %>%
  group_by(sentiment, comment_id) 

setDT(cmvAllcomm)
#make the frame longer so that each sentiment is a column 
cmvAllcomm <- dcast.data.table(cmvAllcomm, comment_id~sentiment, value.var= 'sentiment')

cmvAllcomm <- cmvAllcomm %>%
  left_join(cmvAlldat, by = c("comment_id" = "comment_id")) %>%
  separate(comment_parent_id, into = c("prefix", "comm_parent_id"), sep = "_") %>%
  mutate(length = str_length(comment_body))

cmvDelta <- cmvAllcomm  %>%
  filter(post_flair == "[∆(s) from OP]" & prefix == "t3") #only select posts where the author changed their views and top level comments
cmvDeltacorr <- cmvDelta %>%
  dplyr::select(comment_score, length, joy, trust, anticipation, anger, disgust, fear, sadness)

#graph
correlation_matrix <- round(cor(cmvDeltacorr),1)
corrp.mat <- cor_pmat(cmvDeltacorr) #calculating p-vals

ggcorrplot(correlation_matrix, hc.order =TRUE, 
           type ="upper", p.mat = corrp.mat, insig="blank") 

```
Fear, anger, and anticipation are positively correlated with comment score and getting the OP to change their minds; also slightly longer comments will get higher scores

```{r}
cmvAuthor <- cmvAllcomm %>%
  group_by(author_flair) %>% #authors with delta 
  summarize(count = n())
```

