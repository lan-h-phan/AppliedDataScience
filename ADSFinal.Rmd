---
title: "Applied Data Science Final"
author: "Lan Phan"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)

library(dplyr)
library(tidyverse)
library(ggplot2)
library(readxl)
library(data.table)
library(RColorBrewer)
```

# Tidy Data

A series of steps was taken to compile, clean, and sort the various data sets used in this paper. The attached markdown document reflects the initial process. 

## Import Datasets

As some datasets used full state names while others only had abbreviations, I made an excel spreadsheet called statetranslation in order to connect different datasets based on states. The statetranslation document only had 2 variables, one for full state names, the other is the abbreviation. 

```{r}
statetranslation <- read_xlsx("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/StateTranslation.xlsx")

#US Census Data 2020
censusdat <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/censusclean2020.csv")
censusdat <- censusdat %>%
  mutate(rWhite = White/ PopEstimate,
         rBlack = Black/ PopEstimate,
         rAsian = Asian/ PopEstimate,
         rNative = Native/ PopEstimate,
         rOther = Other/ PopEstimate,
         rMixed = `Inter-racial`/ PopEstimate)

#police violence data
policeviolence <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/policeviolence2018-2021.csv")


#tweet count related to BLM 
tweetcount <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/blmtweetcount2018-2021.csv")

#demonstrations count 2018
c3compiled <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/c3clean.csv")

c3allstate <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/c3clean.csv")
c3allstate <- c3allstate %>%
  mutate(type = if_else(valence %in% c(0,1), 1, 2)) %>% #code if the protest is for or against BLM
  count(state, year, type) %>%
  left_join(statetranslation,
            by = c("state" = "Abbrev")) %>%
  filter(!is.na(state))

c3allstate$Name <- as.factor(c3allstate$Name)

c3allstate <- c3allstate %>%
  filter(year %in% c(2018,2019,2020,2021,2022)) %>%
  pivot_wider(names_from = type, values_from = n)

c3allstate$`2`[is.na(c3allstate$`2`)] <- 1

#maybe this website to add tweet counts: https://www.statology.org/sum-specific-rows-in-r/

```

# Data Visualization

## Descriptive Stats

## Heatmap

```{r}
#packages
library(ComplexHeatmap)
library(hopach) 
library(circlize)
```

```{r}

#function uncentered correlation and average linkage
uncenter.dist<-function(m) {
  as.dist(as.matrix(distancematrix(m, d="cosangle")))
}


best_col = colorRamp2(c(-2, 0, 2), c("#82A3FF", "grey", "#8B2E2E"))


#Setting up= for the 2018 heatmap


pvsum2018 <- policeviolence %>%
  filter(year == 2018) %>%
  group_by(state) %>%
  summarize(PoliceViolence = n())


datsum2018 <- censusdat %>%
  left_join(statetranslation,
             by = c("NAME" = "Name"))

datsum2018 <- datsum2018 %>%
  left_join(pvsum2018,
            by = c("Abbrev" = "state")) %>%
  mutate(
         rPoliceViolence = (PoliceViolence/ PopEstimate)*100) %>%
  dplyr::select(NAME,
         rWhite, rBlack, rNative, rAsian, rOther, rMixed,
         rPoliceViolence)

hm2018dat<-datsum2018[, c(2:8)]

#name the heatmap rows to their states
rownames(hm2018dat) <- datsum2018$NAME

#z-scoring the data
hm2018dat.scale<-scale(hm2018dat)

#uncentered correlation and average linkage

row.clus18<-hclust(uncenter.dist(hm2018dat.scale), method = "ave")
col.clus18<-hclust(uncenter.dist(t(hm2018dat.scale)), method = "ave")

ht2018 = Heatmap(hm2018dat.scale, name = "2018 Heatmap", 
        cluster_rows=row.clus18, cluster_columns=col.clus18, 
        na_col = "white", col = best_col,
        row_names_gp = gpar(fontsize = 5))

```

```{r}
#2019 Heatmap

#Setting up for the 2019 heatmap


pvsum2019 <- policeviolence %>%
  filter(year == 2019) %>%
  group_by(state) %>%
  summarize(PoliceViolence = n())


datsum2019 <- censusdat %>%
  left_join(statetranslation,
             by = c("NAME" = "Name"))

datsum2019 <- datsum2019 %>%
  left_join(pvsum2019,
            by = c("Abbrev" = "state")) %>%
  mutate(
         rPoliceViolence = (PoliceViolence/ PopEstimate)*100) %>%
  dplyr::select(NAME,
         rWhite, rBlack, rNative, rAsian, rOther, rMixed,
         rPoliceViolence)

hm2019dat<-datsum2019[, c(2:8)]

#name the heatmap rows to their states
rownames(hm2019dat) <- datsum2019$NAME

#z-scoring the data
hm2019dat.scale<-scale(hm2019dat)

#uncentered correlation and average linkage

row.clus19<-hclust(uncenter.dist(hm2019dat.scale), method = "ave")
col.clus19<-hclust(uncenter.dist(t(hm2019dat.scale)), method = "ave")

ht2019 = Heatmap(hm2019dat.scale, name = "2019 Heatmap", 
        cluster_rows=row.clus19, cluster_columns=col.clus19, 
        na_col = "white", col = best_col,
        row_names_gp = gpar(fontsize = 5))
```

```{r}
#2020 Heatmap


pvsum2020 <- policeviolence %>%
  filter(year == 2020) %>%
  group_by(state) %>%
  summarize(PoliceViolence = n())


datsum2020 <- censusdat %>%
  left_join(statetranslation,
             by = c("NAME" = "Name"))

datsum2020 <- datsum2020 %>%
  left_join(pvsum2020,
            by = c("Abbrev" = "state")) %>%
  mutate(
         rPoliceViolence = (PoliceViolence/ PopEstimate)*100) %>%
  dplyr::select(NAME,
         rWhite, rBlack, rNative, rAsian, rOther, rMixed,
         rPoliceViolence)

hm2020dat<-datsum2020[, c(2:8)]

#name the heatmap rows to their states
rownames(hm2020dat) <- datsum2020$NAME

#z-scoring the data
hm2020dat.scale<-scale(hm2020dat)

#uncentered correlation and average linkage

row.clus20<-hclust(uncenter.dist(hm2020dat.scale), method = "ave")
col.clus20<-hclust(uncenter.dist(t(hm2020dat.scale)), method = "ave")

ht2020 = Heatmap(hm2020dat.scale, name = "2020 Heatmap", 
        cluster_rows=row.clus20, cluster_columns=col.clus20, 
        na_col = "white", col = best_col,
        row_names_gp = gpar(fontsize = 5))
```

```{r}

ht2018
+ ht2019 + ht2020
```

## Heatmap Maps! 

(officially known as Choropleth)

Parts of the codes are from: https://rdvark.net/2021/12/29/pretty-choropleth-maps-with-sf-and-ggplot2/

For 2018
```{r}
library(leaflet)
library(sf)

#importing the shape of the U.S.
usmap <- read_sf("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/gz_2010_us_040_00_5m.json")

ccc2018 <- c3compiled %>%
  filter(year == 2018)

choroplot18 <- ccc2018 %>%
  group_by(state) %>%
  summarize(count2018 = n()) %>%
  right_join(statetranslation,
             by = c("state" = "Abbrev")) %>%
  inner_join(censusdat,
             by = c("Name" = "NAME")) %>%
  mutate(ratio2018 = (count2018/PopEstimate)*100) %>%
  inner_join(usmap,
             by = c("Name" = "NAME"))

#turn back into a sf object for geom_sf to map out data
choroplot18 <- st_as_sf(choroplot18)


#### Just by count
choroplot18 %>% 
  ggplot(aes(fill = count2018)) + # create a ggplot object and 
  # change its fill colour according to median_age
  geom_sf(colour = NA) +# plot all local authority geometries 
  scale_fill_gradient(low = "#82A3FF", 
                      high = "#8B2E2E", na.value = "white") +
  coord_sf(xlim = c(-180, -60), lims_method = "box") + #just zoom in the U.S.
  theme_void() #remove background

#### By ratio of the population
choroplot18 %>% 
  ggplot(aes(fill = ratio2018)) + # create a ggplot object and 
  # change its fill colour according to median_age
  geom_sf(colour = NA) +# plot all local authority geometries 
  scale_fill_gradient(low = "#82A3FF", 
                      high = "#8B2E2E", na.value = "white") +
  coord_sf(xlim = c(-180, -60), lims_method = "box") + #just zoom in the U.S.
  theme_void() #remove background

#finally omg
#so beautiful imma cry
#also work with plotly
```

## Changes in Demonstrations by Year and States

```{r}

###Font Settings
library(showtext)
library(Hmisc)

font_add_google("Fira Sans", family = "firasans")
font_add_google("Lato", family = "lato")


stategeofacet <- c3allstate %>%
  ggplot(aes(x = year, group = 1)) +
  geom_line(aes(y = `2`, color = "counter-protest")) +
  geom_line(aes(y = `1`, color = "for BLM")) +
  stat_difference(aes(ymin = `1`, ymax = `2`), alpha = .3) +
  facet_geo(vars(state), grid = "us_state_grid1")

stategeofacet <- stategeofacet +
  # Colors for the lines
  scale_color_manual(values = c("#3D85F7", "#C32E5A")) +
  scale_fill_manual(
    values = c(
      colorspace::lighten("#3D85F7"), 
      colorspace::lighten("#C32E5A"),
      "grey60"),
    labels = c("more BLM", "more counter-protest")
  ) +
  # Add labels
  labs(
    title = "Number of demonstrations \nfor and against #BlackLivesMatter \nfrom 2018-2021",
    caption = "Source: CrowdCountingConsortium"
  ) +
  # Specify the order for the guides
  guides(
    # Order indicates the order of each legend among multiple guides.
    # The guide for 'color' will be placed before the onde for 'fill'
    color = guide_legend(order = 1), 
    fill = guide_legend(order = 2)
  ) 



stategeofacet <- stategeofacet +
  # A minimalistic theme with no background annotations
  theme_minimal() +
  theme(
    # Top-right position
    legend.pos = c(0.875, 0.2),
    # Elements within a guide are placed one next to the other in the same row
    legend.direction = "horizontal",
    # Different guides are stacked vertically
    legend.box = "vertical",
    # No legend title
    legend.title = element_blank(),
    # Light background color
    plot.background = element_rect(fill = "#F5F4EF", color = NA),
    plot.margin = margin(20, 30, 20, 30),
    # Customize the title. Note the new font family and its larger size.
    plot.title = element_text(
      margin = margin(0, 0, -50, 0), 
      size = 14, 
      face = "bold", 
      vjust = 0, 
      color = "grey25"
    ),
    plot.caption = element_text(size = 11),
    # Remove titles for x and y axes.
    axis.title = element_blank(),
    # Specify color for the tick labels along both axes 
    axis.text = element_text(color = "grey40", size = 6),
    # Specify face and color for the text on top of each panel/facet
    strip.text = element_text(face = "bold", color = "grey20")
  )

stategeofacet
```


# Data Analysis

## Topic Modeling 

The codes are from 

Bowers, A.J., Chen, J.(2015) Ask and Ye Shall Receive? Automated Text Mining of Michigan Capital Facility Finance Bond Election Proposals to Identify which Topics are Associated with Bond Passage and Voter Turnout. Journal of Education Finance, 41(2), p.164-196. http://dx.doi.org/10.7916/D8FJ2GDH

```{r topic modeling packages}
library(topicmodels)
library(tm)
library(tidytext)
library(tidyr)
library(stringr)
library(slam)
library(ggrepel)
library(MASS)
library(textstem)
library(readtext)
#package to change the time into something that makes sense
library(anytime)
```

### 2018-2019 Reddit Posts
```{r, eval=FALSE}
#Importing Reddit Data for 2018
comments2018 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2018comments.csv", na.string = "")


posts2018 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2018posts.csv", na.string = "")


#reshaping 2018 data by matching post and comments together based on the post_id 
all2018 <- posts2018[comments2018, on = c(id = "submission")] #right_joining 

all2018$time_created <- anydate(all2018$time_created)

#collapsing all comments to join the posts

collapsed18 <- all2018 %>%
  group_by(id) %>%
  summarize(allcomment = paste(comment_body, collapse = " "))

collapsed18 <- collapsed18 %>%
  left_join(posts2018, by = c("id" = "id")) %>%
  filter(!is.na(comms_num)) 

collapsed18$alltext = paste(collapsed18$title, collapsed18$allcomment)

collapsed18$time_created <- anydate(collapsed18$time_created)

#### 2019

posts2019 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2019posts.csv", na.string = "")

comments2019 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2019comments.csv", na.string = "")



#reshaping 2018 data by matching post and comments together based on the post_id 
all2019 <- posts2019[comments2019, on = c(id = "submission")] #right_joining 

all2019$time_created <- anydate(all2019$time_created)

#collapsing all comments to join the posts

collapsed19 <- all2019 %>%
  group_by(id) %>%
  summarize(allcomment = paste(comment_body, collapse = " "))

collapsed19 <- collapsed19 %>%
  left_join(posts2019, by = c("id" = "id")) %>%
  filter(!is.na(comms_num)) 

collapsed19$alltext = paste(collapsed19$title, collapsed19$allcomment)

collapsed19$time_created <- anydate(collapsed19$time_created)

reddit1819 <- rbind(collapsed18, collapsed19)
#fwrite(reddit1819, "reddit1819collapsed.csv")
#posts1819 <- rbind(posts2018, posts2019)
#posts1819$time_created <- anydate(posts1819$time_created)
```

```{r tidy text 18-19, eval = FALSE}

# I did this step in python to clean out names, organization, and location manually
# using the spaCy package

posts1819 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/clean1819all.csv", na.string = "")

posttext1819 <- posts1819 %>%
  unnest_tokens(word, alltext) %>%
  anti_join(stop_words) 

posttext1819 %>%
  group_by(word) %>%
  count(sort = TRUE)

#random select 100 rows cause it is too much 
#ctmdat <- sample_n(discussion2018p, 200)
  
```


*Learning Notes from Dr. Bowers*
topicmodels and tm needs a very specific type of data, called a DocumentTermMatrix. We’ll call specific corpus and DocumentTermMatrix functions here to create the data matrix required for the topic model.

Note here that this is where we stem, stopword, set the min word length, remove numbers, and remove punctuation.

So this sets up our data matrix DTM. We then need to know how to set “alpha” the hyperparameter, so we need some information on the tfidf (term frequency-inverse document frequency)

```{r, eval = FALSE}
#Build CMT
corpus <- Corpus(VectorSource(posts1819$alltext))

#clean using the codes from Wang et al., 2016
REPLACE_STRING <- content_transformer(function(x, pattern, y)
gsub(pattern, y, x, perl = TRUE))

corpus <- tm_map(corpus, REPLACE_STRING, "\\s*â€”\\s*", "") #remove hyphens
corpus <- tm_map(corpus, REPLACE_STRING, "[[:punct:]]", "") #remove punctuations
corpus <- tm_map(corpus, stemDocument) #stemming

corpus <- tm_map(corpus, removeWords, c("cmv", "http", "www", "wiki", "reddit", "com", "police", "officer", "officers", "lives", "matter", "blm", "d't", "n't", "people", "trump", "american", "cops", "americans", "blacklivesmatter", "russian"))


#text manipulation
text_DTM <- DocumentTermMatrix(corpus, control = list(stemming = TRUE, 
                                                      stopwords = TRUE, minWordLength = 3, 
                                                      removeNumbers = TRUE, removePunctuation = TRUE))

term_tfidf <- tapply(text_DTM$v / row_sums(text_DTM)[text_DTM$i], text_DTM$j, mean) * log2(nDocs(text_DTM)/col_sums(text_DTM > 0))

summary(term_tfidf)

plot(density(term_tfidf))
                            
```
Rule of thumb is to use the median from the graph for alpha. Here it's around 1.05
We will cut down the total number of terms by setting alpha so that we don't have too many common terms no unique ones. 
```{r, eval = FALSE}
alpha <- 0.003 #is the median
text_DTM_trimmed <- text_DTM[row_sums(text_DTM) >0, term_tfidf >= alpha]
dim(text_DTM_trimmed)
```

**10-fold Cross Validation**

We set up the 10 fold cross validation, selecting 10% of the data for each of the 10 folds

```{r, eval = FALSE}
# Cross validation

control_CTM_VEM <- list(
  estimate.beta = TRUE, verbose = 0, prefix = tempfile(), save = 0, keep = 0,
  seed = as.integer(Sys.time()), nstart=1L, best = TRUE,
  var = list(iter.max=100, tol=10^-6),
  em = list(iter.max=500, tol=10^-4),
  cg = list(iter.max=100, tol=10^5)
)

# use 10-fold CV to determine k!
# randomly divide the data into 10 folds.

set.seed(123)
topics <- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15) ##set k equals to 2 3 4 5 6 7 8 9 10 15.
seed <- 2
D <- length(posts1819$alltext) 
folding <- sample(rep(seq_len(10), ceiling(D))[seq_len(D)])
table(folding)
```

**Learning Notes**
We train and test using the 10-fold cross validation that we just set up. Then plot by perplexity to see where the elbow is to select the correct number of topics k

Note: I waited for 12 hours (overnight) and it was still not done processing, so I decided to just do it in Python, which took a few minutes. The python code and output are attached in a separate pdf document. However, I will still present the R code as it would have been here. 

```{r, eval=FALSE}

#the code originally did not work because there were columns that had all 0 responses
#so we had to remove them
raw.sum = apply(text_DTM_trimmed, 1, FUN = sum)
text_DTM_trimmed = text_DTM_trimmed[raw.sum!= 0,]


## write a loop to automatically output the perplexity
perp_by_col <- vector()
for (k in topics) {
  perp_by_row <- vector()
  for (chain in seq_len(10)) {
    training <- CTM(text_DTM_trimmed[folding != chain,], k = k,
                    control = control_CTM_VEM)
    testing <- CTM(text_DTM_trimmed[folding == chain,], model = training,
                   control = control_CTM_VEM)
    perp_by_row <- rbind(perp_by_row, perplexity(testing))
  }
  perp_by_col <- cbind(perp_by_col, perp_by_row)
}

 #wow this is indeed taking forever
```

```{r, eval = FALSE}
# Plot perplexity
transpose <- t(perp_by_col)
matplot(transpose, type = "l", col = rainbow(9), lty = 2, lwd = 2, ylab = "Perplexity", xlab = "K", main = "CTM-10-fold cross validation", xaxt="n")
axis(1, at=1:10, labels = c("k=2", "k=3", "k=4", "k=5", "k=6", "k=7", "k=8", "k=9", "k=10", "k=15"), cex=0.5)

perp_by_col_mean <- colMeans(perp_by_col)

lines(perp_by_col_mean, col = "black", lwd = 4, lty = 1)
led <- c("fold=2", "fold=3", "fold=4", "fold=5", "fold=6", "fold=7", "fold=8", "fold=9", "fold=10", "Average")
legend("topright", led, lwd = 2, lty = 2, col = c(rainbow(9), 'black'), cex = 0.65)

abline(v = 4, col = "gray60", lty = 2)
```

```{r, eval = FALSE}
{plot(perp_by_col_mean, pch = 20, ylab = 'Perplexity', xlab = "K", main = "CTM-10-fold cross validation", 
      xaxt = "n") 
  axis(1, at = 1:10, labels = c("k=2","k=3","k=4","k=5","k=6","k=7","k=8","k=9","k=10","k=15"), cex = 0.5)
  lines(perp_by_col_mean, lwd = 1, lty = 2, col = "red")}
```
Alright this is  10 topics

*Correlated Topic Model*
Running 10 correlated topic model
```{r, eval = FALSE}
control_CTM_VEM1 <- list(
  estimate.beta = TRUE, verbose=0, prefix=tempfile(),save=0,keep=0,
  seed=1421313709,nstart=1L,best=TRUE,
  var=list(iter.max=500,tol=10^-6),
  em=list(iter.max=1000,tol=10^-4),
  cg=list(iter.max=500,tol=10^5)
)

################## K = 10 ##################

CTM3 <- CTM(text_DTM_trimmed, k = 10, control = control_CTM_VEM1)
CTM3
```

*Examining the Topic Output*
```{r, eval = FALSE}

## Table the documents by topics
main_topic_table3 <- as.data.frame(table(topics(CTM3)))
colnames(main_topic_table3) <- c("Topic", "Frequency")
print(main_topic_table3)


#what are the top 10 words for the 3 topics
terms(CTM3,10)
```
```{r, eval = FALSE}
# Using this: https://www.tidytextmining.com/topicmodeling.html
# Use tidyverse to look at the CTM results a bit more more

tidy_topics <- tidy(CTM3, matrix = "beta")

top_terms <- tidy_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta) 

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_fill_brewer(palette = "Paired") +
  scale_y_reordered() +
  theme_bw()
```
```{r, eval = FALSE}

## Topics
topics3 <- posterior(CTM3)$topics
## Let's look at the probability of each document info fits into each of the topics
topics3 <- as.data.frame(topics3)
nrow(topics3)
rownames(topics3) <- posts1819$name
print(topics3)
## Let's look at which topic each document is assigned to one of the topics.
main_topic3 <- as.data.frame(topics(CTM3))
rownames(main_topic3) <- posts1819$doc_id
colnames(main_topic3) <- "Main_Topic"
print(main_topic3)
```

### 2020-2021 Posts
```{r}
posts2021 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2021tm.csv", na.string = "")
posts2021$time_created <- anydate(posts2021$time_created)
colnames(posts2021)
  
```

```{r}
#Build CMT
corpus2 <- Corpus(VectorSource(posts2021$topictext))


#text manipulation
text_DTM2 <- DocumentTermMatrix(corpus2, control = list(stemming = TRUE, stopwords = TRUE, minWordLength = 3, 
                                                      removeNumbers = TRUE, removePunctuation = TRUE))
text_DTM2

term_tfidf2 <- tapply(text_DTM2$v / row_sums(text_DTM2)[text_DTM2$i], text_DTM2$j, mean) * log2(nDocs(text_DTM2)/col_sums(text_DTM2 > 0))
summary(term_tfidf2)

plot(density(term_tfidf2))
                            
```
Rule of thumb is to use the median from the graph for alpha. Here it's around 0.7.
We will cut down the total number of terms by setting alpha so that we don't have too many common terms no unique ones. 
```{r}
alpha2 <- 0.003 #is the median
text_DTM_trimmed2 <- text_DTM2[row_sums(text_DTM2) >0, term_tfidf2 >= alpha2]
dim(text_DTM_trimmed2)
```

*10-fold Cross Validation*

We set up the 10 fold cross validation, selecting 10% of the data for each of the 10 folds

```{r}
# Cross validation

control_CTM_VEM <- list(
  estimate.beta = TRUE, verbose = 0, prefix = tempfile(), save = 0, keep = 0,
  seed = as.integer(Sys.time()), nstart=1L, best = TRUE,
  var = list(iter.max=100, tol=10^-6),
  em = list(iter.max=500, tol=10^-4),
  cg = list(iter.max=100, tol=10^5)
)

# use 10-fold CV to determine k!
# randomly divide the data into 10 folds.

set.seed(123)
topics <- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15) ##set k equals to 2 3 4 5 6 7 8 9 10 15.
seed <- 2
D2 <- length(posts2021$topictext) 
folding <- sample(rep(seq_len(10), ceiling(D2))[seq_len(D2)])
table(folding)
```

*Learning Notes*
We train and test using the 10-fold cross validation that we just set up. Then plot by perplexity to see where the elbow is to select the correct number of topics k

Note: This can take a bit of time.

```{r}


#the code originally did not work because there were columns that had all 0 responses
#so we had to remove them
raw.sum2 = apply(text_DTM_trimmed2, 1, FUN = sum)
text_DTM_trimmed2 = text_DTM_trimmed2[raw.sum2!= 0,]


## write a loop to automatically output the perplexity
perp_by_col <- vector()
for (k in topics) {
  perp_by_row <- vector()
  for (chain in seq_len(10)) {
    training <- CTM(text_DTM_trimmed2[folding != chain,], k = k,
                    control = control_CTM_VEM)
    testing <- CTM(text_DTM_trimmed2[folding == chain,], model = training,
                   control = control_CTM_VEM)
    perp_by_row <- rbind(perp_by_row, perplexity(testing))
  }
  perp_by_col <- cbind(perp_by_col, perp_by_row)
}

```

```{r}
# Plot perplexity
transpose <- t(perp_by_col)
matplot(transpose, type = "l", col = rainbow(9), lty = 2, lwd = 2, ylab = "Perplexity", xlab = "K", main = "CTM-10-fold cross validation", xaxt="n")
axis(1, at=1:10, labels = c("k=2", "k=3", "k=4", "k=5", "k=6", "k=7", "k=8", "k=9", "k=10", "k=15"), cex=0.5)

perp_by_col_mean <- colMeans(perp_by_col)

lines(perp_by_col_mean, col = "black", lwd = 4, lty = 1)
led <- c("fold=2", "fold=3", "fold=4", "fold=5", "fold=6", "fold=7", "fold=8", "fold=9", "fold=10", "Average")
legend("topright", led, lwd = 2, lty = 2, col = c(rainbow(9), 'black'), cex = 0.65)

abline(v = 4, col = "gray60", lty = 2)
```

```{r}
{plot(perp_by_col_mean, pch = 20, ylab = 'Perplexity', xlab = "K", main = "CTM-10-fold cross validation", 
      xaxt = "n") 
  axis(1, at = 1:10, labels = c("k=2","k=3","k=4","k=5","k=6","k=7","k=8","k=9","k=10","k=15"), cex = 0.5)
  lines(perp_by_col_mean, lwd = 1, lty = 2, col = "red")}
```
From python, 10 topics

*Correlated Topic Model*
Running 10 correlated topic model
```{r}
control_CTM_VEM1 <- list(
  estimate.beta = TRUE, verbose=0, prefix=tempfile(),save=0,keep=0,
  seed=1421313709,nstart=1L,best=TRUE,
  var=list(iter.max=500,tol=10^-6),
  em=list(iter.max=1000,tol=10^-4),
  cg=list(iter.max=500,tol=10^5)
)

################## K = 9 ##################

CTM4 <- CTM(text_DTM_trimmed2, k = 10, control = control_CTM_VEM1)
CTM4
```

*Examining the Topic Output*
```{r}

## Table the documents by topics
main_topic_table3 <- as.data.frame(table(topics(CTM4)))
colnames(main_topic_table3) <- c("Topic", "Frequency")
print(main_topic_table3)


#what are the top 10 words for the 3 topics
terms(CTM4,10)
```
```{r}
# Using this: https://www.tidytextmining.com/topicmodeling.html
# Use tidyverse to look at the CTM results a bit more more

tidy_topics <- tidy(CTM4, matrix = "beta")

top_terms <- tidy_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta) 

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_fill_brewer(palette = "BuPu") +
  scale_y_reordered() +
  theme_bw()
```

## Sentiment Analysis
```{r packages for sentiment}
library(tidytext)
library(textdata)
nrc <- get_sentiments("nrc") 
```

```{r data 20-21 import}
reddit2021 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2021tm.csv", na.string = "")
reddit2021$time_created <- anydate(reddit2021$time_created)
colnames(reddit2021)
reddit2021 <- reddit2021 %>%
  dplyr::select(id, allcomment, time_created, title, post_flair, score, upvote_ratio, comms_num, created, body, author, author_flair, subreddit, alltext, topictext, Topic)
```

```{r tidy data}
tidy_r2021 <- reddit2021 %>%
  unnest_tokens(word, topictext)
```
Sentiment Analysis
```{r}
tidy_r2021 %>%
  inner_join(nrc) %>%
  group_by(sentiment, Topic) %>%
  count(sort = TRUE)
```

```{r sentiment plot}

tidy_r2021 %>%
  inner_join(get_sentiments("afinn")) %>%
  separate(time_created, into = c("year", "month", "date"), sep = "-", remove = FALSE) %>%
  unite(ym, year, month, sep = "-") %>%
  group_by(time_created) %>%
  summarise(sentiment = sum(value),
            Topic = Topic,
            index = ym) %>%
  ggplot(aes(index, sentiment, fill = Topic)) +
  geom_col(show.legend = FALSE) +
  xlab("")+
  facet_wrap(~Topic, ncol = 3) +
  theme_bw()
```

```{r}
tidy_r2021 %>%
  inner_join(get_sentiments("bing")) %>%
  separate(time_created, into = c("year", "month", "date"), sep = "-", remove = FALSE) %>%
  unite(ym, year, month, sep = "-") %>%
  group_by(time_created) %>%
  count(Topic, index = ym, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative) %>%
  ggplot(aes(index, sentiment, fill = Topic)) +
  geom_col(show.legend = FALSE) +
  xlab("")+
  facet_wrap(~Topic, ncol = 3) +
  theme_bw()
```
What if we study the texts with higher engagement vs. lower engagement
Run correlations?
```{r}
nrc_small <- get_sentiments("nrc") %>%
  filter(sentiment %in% c("trust", "anticipation", "joy", 
                        "fear", "anger", "disgust", "sadness"))
summary(reddit2021$comms_num) #split at the median of 33

#just post titles
res2021 <- reddit2021 %>%
  unnest_tokens(word, title) %>%
  inner_join(nrc_small) %>%
  group_by(sentiment, id)# %>%
  #mutate(count = n())

setDT(res2021)
#make the frame longer so that each sentiment is a column 
res2021 <- dcast.data.table(res2021, id~sentiment, value.var= 'sentiment')

res2021 <- res2021 %>%
  inner_join(reddit2021, by = c("id" = "id"))
#rm(reddit2021) #for now as vector memory exhausted
colnames(res2021)
res2021corr <- res2021 %>%
  dplyr::select(comms_num, score, upvote_ratio, joy, trust, anticipation, anger, disgust, fear, sadness)

library(ggcorrplot)
correlation_matrix <- round(cor(res2021corr),1)
corrp.mat <- cor_pmat(res2021corr) #calculating p-vals

ggcorrplot(correlation_matrix, hc.order =TRUE, 
           type ="upper", p.mat = corrp.mat, insig="blank") 

```
Fear and trust sentiments in the title are positively correlated with number of comments and upvote ratio

Disgust and sadness are negatively correlated with upvote ratio

What about comments?

```{r}
posts2020 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2020posts.csv", na.string = "")
posts2021 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2021posts.csv", na.string = "")
reddit2021p <- rbind(posts2020, posts2021)
reddit2021p <- reddit2021p[comms_num > 33,]

comments2020 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2020comments.csv", na.string = "")
comments2021 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2021comments.csv", na.string = "")
reddit2021c <- rbind(comments2020, comments2021)


#just take comments from posts with more than 2 comments
reddit2021c <- reddit2021c %>%
  inner_join(reddit2021p, by = c("submission" = "id"))
reddit2021c <- reddit2021c[!is.na(comment_body),]

colnames(reddit2021c)
```


```{r}

afinn <- get_sentiments("afinn") 
 # summarise(sentiment = sum(value),
          


comm2021 <- reddit2021c %>%
  unnest_tokens(word, comment_body) %>%
  inner_join(nrc_small) %>%
  group_by(sentiment, comment_id)

setDT(comm2021)
#make the frame longer so that each sentiment is a column 
comm2021 <- dcast.data.table(comm2021, comment_id~sentiment, value.var= 'sentiment')

comm2021 <- comm2021 %>%
  inner_join(reddit2021c, by = c("comment_id" = "comment_id")) %>%
  separate(comment_parent_id, into = c("prefix", "comm_parent_id"), sep = "_") 

comm2021 <- comm2021  %>%
  filter(prefix == "t3") #only select top level comments
comm2021corr <- comm2021 %>%
  dplyr::select(comment_score, joy, trust, anticipation, anger, disgust, fear, sadness)

#graph
correlation_matrix <- round(cor(comm2021corr),1)
corrp.mat <- cor_pmat(comm2021corr) #calculating p-vals

ggcorrplot(correlation_matrix, hc.order =TRUE, 
           type ="upper", p.mat = corrp.mat, insig="blank") 
```

#still nothing
