---
title: "Applied Data Science Final"
author: "Lan Phan"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)

library(dplyr)
library(tidyverse)
library(ggplot2)
library(readxl)
library(data.table)
library(RColorBrewer)
library(ggcorrplot)
library(plotly)
library(lubridate)
```

# Tidy Data

A series of steps was taken to compile, clean, and sort the various data sets used in this paper. The below codes reflects the initial process. 

```{r US Census, eval = FALSE}
#US Census Cleaning

#race census 2020
racecensus <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/Resonance/BLM/Racecensus.csv")

#turn statenames to a column
racecensus <- racecensus %>%
  pivot_longer(c('Alabama':'Puerto Rico'), names_to = "STATE", values_to = "count")
#labels into column names
racecensus <- racecensus %>%
  pivot_wider(names_from = `Label (Grouping)`, values_from = count)

racecensus <- racecensus %>%
  rename_at(2, ~ "Total") %>%
  rename_at(4, ~ "White") %>%
  rename_at(5, ~ "Black") %>%
  rename_at(6, ~ "Native") %>%
  rename_at(7, ~ "Asian") %>%
  rename_at(8, ~ "Native Hawaiian/PI") %>%
  rename_at(9, ~ "Other") %>%
  rename_at(10, ~ "Inter-racial")

racecensus <- racecensus %>%
  select(1:11)

#save this smaller data set
write_csv(racecensus, "racecensus2020.csv")

#general number count
racecensus <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/Resonance/BLM/racecensus2020.csv")

censusraw <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/Resonance/BLM/Census20202021.csv")

censusraw <- censusraw %>%
  rename_at(7, ~ "PopEstimate") %>%
  select(1:7)

censusclean <- censusraw %>%
  inner_join(racecensus,
            by = c("NAME" = "STATE"))

write_csv(censusclean, "censusclean2020.csv")

```

```{r Police Violence, eval = FALSE}
#Police Violence

statetranslation <- read_xlsx("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/StateTranslation.xlsx")

policeviolence <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/Resonance/BLM/Mapping Police Violence-Grid view.csv")

policeviolence <- policeviolence %>%
  select(age, gender, race, date, city, state, county,
         agency_responsible, cause_of_death,
         circumstances, officer_charged, signs_of_mental_illness,
         allegedly_armed, wapo_armed,
         wapo_flee, wapo_body_camera,
         encounter_type, initial_reason,
         call_for_service,
         pop_white_census_tract, pop_black_census_tract,
         pop_hispanic_census_tract) %>%
  separate(date, into = c("month", "day", "year"), sep = "/") %>%
  left_join(statetranslation,
            by = c("state" = "Abbrev")) 

write_csv(policeviolence, "policeviolence2018-2021.csv")
```

```{r Tweet Count, eval = FALSE}
#Tweet Count Cleaning

tweetcount <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/Resonance/BLM/tweet_counts_per_day.csv")

#just tweet counts from 2018-2021
tweetcount <- tweetcount %>%
  separate(date_str, into = c("year", "month", "date"), sep = "-") %>%
  filter(year %in% c("2018", "2019", "2020", "2021"))

write_csv(tweetcount, "blmtweetcount2018-2021.csv")
```

```{r Demonstration & Bridging Org, eval = FALSE}
# Demonstrations Count Cleaning
#Probably should sort into for BlackLivesMatter; against BLM (i.e. AllLivesMatter); anti and pro-police/ BlueLivesMatter

# Thank you to this person who cleaned this up
# https://github.com/nonviolent-action-lab/crowd-counting-consortium

c3compiled <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/Resonance/BLM/ccc_compiled.csv")

blmkeywords <- c("policing", "racism")

blmkeywords <- str_c(blmkeywords, collapse = "|")


c3compiled <- c3compiled %>%
  separate(date, into = c("year", "month", "date"), sep = "-") %>%
  filter(year %in% c("2018", "2019", "2020", "2021", "2022")) %>%
  filter(grepl("policing|racism", issues, ignore.case = TRUE))

write_csv(c3compiled, "c3clean.csv")

# Bridge building organizations list
bdidat <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/Resonance/BLM/bridging-organizations-data-2022-11-21.csv")
#probably not going to filter this 
#addressing one aspects might have a ripple effect on others
#cooperative vs competitive (demonstrations are more focused on race)
```

## Import Datasets

As some datasets used full state names while others only had abbreviations, I made an excel spreadsheet called statetranslation in order to connect different datasets based on states. The statetranslation document only had 2 variables, one for full state names, the other is the abbreviation. 

```{r}
statetranslation <- read_xlsx("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/StateTranslation.xlsx")

#US Census Data 2020
censusdat <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/censusclean2020.csv")
censusdat <- censusdat %>%
  mutate(rWhite = White/ PopEstimate,
         rBlack = Black/ PopEstimate,
         rAsian = Asian/ PopEstimate,
         rNative = Native/ PopEstimate,
         rOther = Other/ PopEstimate,
         rMixed = `Inter-racial`/ PopEstimate)

#demonstrations count 2018
c3compiled <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/c3clean.csv")

c3allstate <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/c3clean.csv")
c3allstate <- c3allstate %>%
  mutate(type = if_else(valence %in% c(0,1), 1, 2)) %>% #code if the protest is for or against BLM
  count(state, year, type) %>%
  left_join(statetranslation,
            by = c("state" = "Abbrev")) %>%
  filter(!is.na(state))

c3allstate$Name <- as.factor(c3allstate$Name)

c3allstate <- c3allstate %>%
  filter(year %in% c(2018,2019,2020,2021,2022)) %>%
  pivot_wider(names_from = type, values_from = n)

c3allstate$`2`[is.na(c3allstate$`2`)] <- 1


#maybe this website to add tweet counts: https://www.statology.org/sum-specific-rows-in-r/

```

# Data Visualization
```{r font}
###Font Settings for graphs
library(showtext)
font_add_google("Fira Sans", family = "firasans")
font_add_google("Lato", family = "lato")

font_add_google("Fira Sans", family = "firasans")
font_add_google("Lato", family = "lato")
```

## Descriptive Stats
Summaries of different datasets are embedded throughout the markdown document.

## Heatmap Maps! 

(officially known as Choropleth)

Parts of the codes are from: https://rdvark.net/2021/12/29/pretty-choropleth-maps-with-sf-and-ggplot2/
Maybe do this for bridge-building orgs
```{r}
library(leaflet)
library(sf)


#importing the shape of the U.S.
usmap <- read_sf("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/gz_2010_us_040_00_5m.json")

#Bridge-building Organizations
bdidat <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/Resonance/BLM/bridging-organizations-data-2022-11-21.csv")


choroplot <- bdidat %>%
  group_by(state) %>%
  summarize(count = n()) %>%
  left_join(statetranslation, by = c("state" = "Name")) %>%
  inner_join(censusdat,
             by = c("state" = "NAME")) %>%
  mutate(ratio = (count/PopEstimate)*100) %>%
  inner_join(usmap,
             by = c("state" = "NAME"))

#turn back into a sf object for geom_sf to map out data
choroplot <- st_as_sf(choroplot)


#### Just by count
choroplot %>% 
  ggplot(aes(fill = count)) + # create a ggplot object and 
  # change its fill colour according to median_age
  geom_sf(colour = NA) +# plot all local authority geometries 
  scale_fill_gradient(low = "#82A3FF", 
                      high = "#8B2E2E", na.value = "white") +
  coord_sf(xlim = c(-180, -60), lims_method = "box") + #just zoom in the U.S.
  theme_void() +
  theme(
    # Top-right position
    #legend.pos = c(0.875, 0.2),
    # No legend title
    legend.title = element_blank(),
    legend.text = element_text(color = "grey20", size = 8),
    # Light background color
    plot.background = element_rect(fill = "#F5F4EF", color = NA),
    plot.margin = margin(20, 30, 20, 30),
    plot.caption = element_text(size = 11),
    # Remove titles for x and y axes.
    axis.title = element_blank(),
    # Specify color for the tick labels along both axes 
    axis.text = element_text(color = "grey40", size = 6),
    # Specify face and color for the text on top of each panel/facet
    strip.text = element_text(face = "bold", color = "grey20")
  )

#  theme(plot.background = element_rect(fill = "#F5F4EF", color = NA))


#also work with plotly
#ggplotly(ggchoroplot)


#finally omg
#so beautiful imma cry

```

## Changes of Tweets and Police Violence Over Time

```{r line plot}
library(geofacet)
library(ggh4x)


#tweet count related to BLM 
tweetcount <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/blmtweetcount2018-2021.csv")

tweetcount <- tweetcount %>%
  group_by(year, month) %>%
  dplyr::summarize(blm = sum(blm, na.rm = TRUE),
            alm = sum(alm, na.rm = TRUE),
            blulm = sum(blulm, na.rm = TRUE)) %>%
  mutate(against = alm + blulm) %>%
  unite(ym, year, month, sep = "-", remove = FALSE) 


#all dat
tweetcount %>%
  ggplot(aes(x = ym, group = 1)) +
  geom_line(aes(y = blm, color = "#BlackLivesMatter")) +
  geom_line(aes(y = against, color = "#All/BlueLivesMatter")) +
  stat_difference(aes(ymin = against, ymax = blm), alpha = 0.3) +
  scale_color_manual(values = c("#C32E5A", "#3D85F7")) +
  scale_fill_manual(
    values = c(
      colorspace::lighten("#3D85F7"), 
      colorspace::lighten("#C32E5A"),
      "grey60"),
    labels = c("more BLM", "more counter-protest")
  ) +
  # Add labels
  labs(
    title = "Tweet count for and against #BlackLivesMatter, 2018-2019"
  ) +
  scale_x_discrete(breaks = NULL, labels = NULL) +
  # Specify the order for the guides
  guides(
    # Order indicates the order of each legend among multiple guides.
    # The guide for 'color' will be placed before the one for 'fill'
    color = guide_legend(order = 1), 
    fill = guide_legend(order = 2)
  ) +
  theme_minimal() +
  theme(
    # Top-right position
    legend.pos = c(0.875, 0.2),
    # Elements within a guide are placed one next to the other in the same row
    legend.direction = "horizontal",
    # Different guides are stacked vertically
    legend.box = "vertical",
    # No legend title
    legend.title = element_blank(),
    # Light background color
    plot.background = element_rect(fill = "#F5F4EF", color = NA),
    plot.margin = margin(20, 30, 20, 30),
    # Customize the title. Note the new font family and its larger size.
    plot.title = element_text(
      margin = margin(0, 0, -50, 0), 
      size = 12, 
      face = "bold", 
      vjust = 0, 
      color = "grey25"
    ),
    plot.caption = element_text(size = 11),
    # Remove titles for x and y axes.
    axis.title = element_blank(),
    # Specify color for the tick labels along both axes 
    axis.text = element_text(color = "grey40", size = 6),
    # Specify face and color for the text on top of each panel/facet
    strip.text = element_text(face = "bold", color = "grey20")
  )

```
```{r}

###Just 2018-2019 
tweetcount %>%
  filter(year %in% c("2018","2019")) %>%
  ggplot(aes(x = ym, group = 1)) +
  geom_line(aes(y = blm, color = "#BlackLivesMatter")) +
  geom_line(aes(y = against, color = "#All/BlueLivesMatter")) +
  stat_difference(aes(ymin = against, ymax = blm), alpha = 0.3) +
  scale_color_manual(values = c("#C32E5A", "#3D85F7")) +
  scale_fill_manual(
    values = c(
      colorspace::lighten("#3D85F7"), 
      colorspace::lighten("#C32E5A"),
      "grey60"),
    labels = c("more BLM", "more tweets against")
  ) +
  # Add labels
  labs(
    title = "Tweet count for and against #BlackLivesMatter, 2018-2019"
  ) +
  scale_x_discrete(breaks = NULL, labels = NULL) +
  # Specify the order for the guides
  guides(
    # Order indicates the order of each legend among multiple guides.
    # The guide for 'color' will be placed before the one for 'fill'
    color = guide_legend(order = 1), 
    fill = guide_legend(order = 2)
  ) +
  theme_minimal() +
  theme(
    # Top-right position
    legend.pos = c(0.875, 0.2),
    # Elements within a guide are placed one next to the other in the same row
    legend.direction = "horizontal",
    # Different guides are stacked vertically
    legend.box = "vertical",
    # No legend title
    legend.title = element_blank(),
    # Light background color
    plot.background = element_rect(fill = "#F5F4EF", color = NA),
    plot.margin = margin(20, 30, 20, 30),
    # Customize the title. Note the new font family and its larger size.
    plot.title = element_text(
      margin = margin(0, 0, -50, 0), 
      size = 12, 
      face = "bold", 
      vjust = 0, 
      color = "grey25"
    ),
    plot.caption = element_text(size = 11),
    # Remove titles for x and y axes.
    axis.title = element_blank(),
    # Specify color for the tick labels along both axes 
    axis.text = element_text(color = "grey40", size = 6),
    # Specify face and color for the text on top of each panel/facet
    strip.text = element_text(face = "bold", color = "grey20")
  )
```
```{r}

#just 2020-2021
tweetcount %>%
  filter(year %in% c("2020","2021")) %>%
  ggplot(aes(x = ym, group = 1)) +
  geom_line(aes(y = blm, color = "#BlackLivesMatter")) +
  geom_line(aes(y = against, color = "#All/BlueLivesMatter")) +
  stat_difference(aes(ymin = against, ymax = blm), alpha = 0.3) +
  scale_color_manual(values = c("#C32E5A", "#3D85F7")) +
  scale_fill_manual(
    values = c(
      colorspace::lighten("#3D85F7"), 
      colorspace::lighten("#C32E5A"),
      "grey60"),
    labels = c("more BLM", "more tweets against")
  ) +
  # Add labels
  labs(
    title = "Tweet count for and against #BlackLivesMatter, 2020-2021"
  ) +
  scale_x_discrete(breaks = NULL, labels = NULL) +
  # Specify the order for the guides
  guides(
    # Order indicates the order of each legend among multiple guides.
    # The guide for 'color' will be placed before the one for 'fill'
    color = guide_legend(order = 1), 
    fill = guide_legend(order = 2)
  ) +
  theme_minimal() +
  theme(
    # Top-right position
    legend.pos = c(0.875, 0.2),
    # Elements within a guide are placed one next to the other in the same row
    legend.direction = "horizontal",
    # Different guides are stacked vertically
    legend.box = "vertical",
    # No legend title
    legend.title = element_blank(),
    # Light background color
    plot.background = element_rect(fill = "#F5F4EF", color = NA),
    plot.margin = margin(20, 30, 20, 30),
    # Customize the title. Note the new font family and its larger size.
    plot.title = element_text(
      margin = margin(0, 0, -50, 0), 
      size = 12, 
      face = "bold", 
      vjust = 0, 
      color = "grey25"
    ),
    plot.caption = element_text(size = 11),
    # Remove titles for x and y axes.
    axis.title = element_blank(),
    # Specify color for the tick labels along both axes 
    axis.text = element_text(color = "grey40", size = 6),
    # Specify face and color for the text on top of each panel/facet
    strip.text = element_text(face = "bold", color = "grey20")
  ) 

```



## Changes in Demonstrations by Year and States

Code adapted from Georgios Karamanis from Tidy Tuesday Challenge https://github.com/gkaramanis/tidytuesday/blob/master/2021/2021-week26/animal-rescues.R
```{r}

###Font Settings
library(showtext)
library(Hmisc)



font_add_google("Fira Sans", family = "firasans")
font_add_google("Lato", family = "lato")

#police violence data
policeviolence <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/policeviolence2018-2021.csv")

policeviolence <- policeviolence %>%
  group_by(Name, year) %>%
  summarise(pvcount = n())

c3allstate <- c3allstate %>%
  left_join(policeviolence, by = c("Name" = "Name",
                                   "year" = "year")) %>%
  filter(year %in% c(2018,2019,2020,2021)) 


stategeofacet <- c3allstate %>%
  ggplot(aes(x = year, group = 1)) +
  geom_line(aes(y = `2`, color = "counter-protest")) +
  geom_line(aes(y = `1`, color = "for BLM")) +
  geom_line(aes(y = `pvcount`, color = "police violence")) +
  #stat_difference(aes(ymin = `1`, ymax = `2`), alpha = .3) +
  facet_geo(vars(state), strip.position = "bottom", grid = "us_state_grid1")

stategeofacet <- stategeofacet +
  # Colors for the lines
  scale_color_manual(values = c("#C32E5A", "#3D85F7", "grey60")) +
  #scale_fill_manual(
   # values = c(
    #  colorspace::lighten("#C32E5A"), 
    #  colorspace::lighten("#3D85F7"),
    #  "grey60"),
   # labels = c("more BLM", "more counter-protest")
  #) +
  # Add labels
  labs(
    title = "Demonstrations for and against \n #BlackLivesMatter and Police Violence \nfrom 2018-2021",
    caption = "Source: CrowdCountingConsortium"
  ) +
  scale_x_discrete(breaks = NULL, labels = NULL) +
  # Specify the order for the guides
  guides(
    # Order indicates the order of each legend among multiple guides.
    # The guide for 'color' will be placed before the onde for 'fill'
    color = guide_legend(order = 1)#, 
    #fill = guide_legend(order = 2)
  ) 



stategeofacet <- stategeofacet +
  # A minimalistic theme with no background annotations
  theme_minimal() +
  theme(
    # Top-right position
    legend.pos = c(0.875, 0.2),
    # Elements within a guide are placed one next to the other in the same row
    legend.direction = "horizontal",
    # Different guides are stacked vertically
    legend.box = "vertical",
    # No legend title
    legend.title = element_blank(),
    # Light background color
    plot.background = element_rect(fill = "#F5F4EF", color = NA),
    plot.margin = margin(20, 30, 20, 30),
    # Customize the title. Note the new font family and its larger size.
    plot.title = element_text(
      margin = margin(0, 0, -50, 0), 
      size = 14, 
      face = "bold", 
      vjust = 0, 
      color = "grey25"
    ),
    plot.caption = element_text(size = 11),
    # Remove titles for x and y axes.
    axis.title = element_blank(),
    # Specify color for the tick labels along both axes 
    axis.text = element_text(color = "grey40", size = 6),
    # Specify face and color for the text on top of each panel/facet
    strip.text = element_text(face = "bold", color = "grey20")
  )

stategeofacet
```


# Data Analysis

## Topic Modeling 

Because of historical effects, per Dr. Bowers' recommendations, I will group 2018-2019 data together and compare them against 2020-2021 data. As discussed in class, the dataset is too large for R to realistically run in time. As a result, I had to run analyses with Python on Google Colab. 

However, I have used the codes below to run smaller subset to test, so I will still present the R codes in this document without running them. 

The codes are from:

Bowers, A.J., Chen, J.(2015) Ask and Ye Shall Receive? Automated Text Mining of Michigan Capital Facility Finance Bond Election Proposals to Identify which Topics are Associated with Bond Passage and Voter Turnout. Journal of Education Finance, 41(2), p.164-196. http://dx.doi.org/10.7916/D8FJ2GDH

```{r topic modeling packages}
library(topicmodels)
library(tm)
library(tidytext)
library(tidyr)
library(stringr)
library(slam)
library(ggrepel)
library(MASS)
library(textstem)
library(readtext)
#package to change the time into something that makes sense
library(anytime)
```

### 2018-2019 Reddit Posts
```{r, eval=FALSE}
#Importing Reddit Data for 2018
comments2018 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2018comments.csv", na.string = "")


posts2018 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2018posts.csv", na.string = "")


#reshaping 2018 data by matching post and comments together based on the post_id 
all2018 <- posts2018[comments2018, on = c(id = "submission")] #right_joining 

all2018$time_created <- anydate(all2018$time_created)

#collapsing all comments to join the posts

collapsed18 <- all2018 %>%
  group_by(id) %>%
  summarize(allcomment = paste(comment_body, collapse = " "))

collapsed18 <- collapsed18 %>%
  left_join(posts2018, by = c("id" = "id")) %>%
  filter(!is.na(comms_num)) 

collapsed18$alltext = paste(collapsed18$title, collapsed18$allcomment)

collapsed18$time_created <- anydate(collapsed18$time_created)

#### 2019

posts2019 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2019posts.csv", na.string = "")

comments2019 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2019comments.csv", na.string = "")



#reshaping 2018 data by matching post and comments together based on the post_id 
all2019 <- posts2019[comments2019, on = c(id = "submission")] #right_joining 

all2019$time_created <- anydate(all2019$time_created)

#collapsing all comments to join the posts

collapsed19 <- all2019 %>%
  group_by(id) %>%
  summarize(allcomment = paste(comment_body, collapse = " "))

collapsed19 <- collapsed19 %>%
  left_join(posts2019, by = c("id" = "id")) %>%
  filter(!is.na(comms_num)) 

collapsed19$alltext = paste(collapsed19$title, collapsed19$allcomment)

collapsed19$time_created <- anydate(collapsed19$time_created)

reddit1819 <- rbind(collapsed18, collapsed19)

reddit1819all <- rbind(all2018, all2019)
#fwrite(reddit1819all, "reddit1819all.csv")
#posts1819 <- rbind(posts2018, posts2019)
#posts1819$time_created <- anydate(posts1819$time_created)
```

```{r tidy text 18-19, eval = FALSE}

# I did this step in python to clean out names, organization, and location manually
# using the spaCy package

posts1819 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/clean1819all.csv", na.string = "")

```


*Learning Notes from Dr. Bowers*
topicmodels and tm needs a very specific type of data, called a DocumentTermMatrix. We’ll call specific corpus and DocumentTermMatrix functions here to create the data matrix required for the topic model.

Note here that this is where we stem, stopword, set the min word length, remove numbers, and remove punctuation.

So this sets up our data matrix DTM. We then need to know how to set “alpha” the hyperparameter, so we need some information on the tfidf (term frequency-inverse document frequency)

```{r, eval = FALSE}
#Build CMT
corpus <- Corpus(VectorSource(posts1819$alltext))

#clean using the codes from Wang et al., 2016
REPLACE_STRING <- content_transformer(function(x, pattern, y)
gsub(pattern, y, x, perl = TRUE))

corpus <- tm_map(corpus, REPLACE_STRING, "\\s*â€”\\s*", "") #remove hyphens
corpus <- tm_map(corpus, REPLACE_STRING, "[[:punct:]]", "") #remove punctuations
corpus <- tm_map(corpus, stemDocument) #stemming

corpus <- tm_map(corpus, removeWords, c("cmv", "http", "www", "wiki", "reddit", "com", "police", "officer", "officers", "lives", "matter", "blm", "d't", "n't", "people", "trump", "american", "cops", "americans", "blacklivesmatter", "russian"))


#text manipulation
text_DTM <- DocumentTermMatrix(corpus, control = list(stemming = TRUE, 
                                                      stopwords = TRUE, minWordLength = 3, 
                                                      removeNumbers = TRUE, removePunctuation = TRUE))

term_tfidf <- tapply(text_DTM$v / row_sums(text_DTM)[text_DTM$i], text_DTM$j, mean) * log2(nDocs(text_DTM)/col_sums(text_DTM > 0))

summary(term_tfidf)

plot(density(term_tfidf))
                            
```
Rule of thumb is to use the median from the graph for alpha. Here it's around 1.05
We will cut down the total number of terms by setting alpha so that we don't have too many common terms no unique ones. 
```{r, eval = FALSE}
alpha <- 0.003 #is the median
text_DTM_trimmed <- text_DTM[row_sums(text_DTM) >0, term_tfidf >= alpha]
dim(text_DTM_trimmed)
```

**10-fold Cross Validation**

We set up the 10 fold cross validation, selecting 10% of the data for each of the 10 folds

```{r, eval = FALSE}
# Cross validation

control_CTM_VEM <- list(
  estimate.beta = TRUE, verbose = 0, prefix = tempfile(), save = 0, keep = 0,
  seed = as.integer(Sys.time()), nstart=1L, best = TRUE,
  var = list(iter.max=100, tol=10^-6),
  em = list(iter.max=500, tol=10^-4),
  cg = list(iter.max=100, tol=10^5)
)

# use 10-fold CV to determine k!
# randomly divide the data into 10 folds.

set.seed(123)
topics <- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15) ##set k equals to 2 3 4 5 6 7 8 9 10 15.
seed <- 2
D <- length(posts1819$alltext) 
folding <- sample(rep(seq_len(10), ceiling(D))[seq_len(D)])
table(folding)
```

**Learning Notes**
We train and test using the 10-fold cross validation that we just set up. Then plot by perplexity to see where the elbow is to select the correct number of topics k

Note: I waited for 12 hours (overnight) and it was still not done processing, so I decided to just do it in Python, which took a few minutes. The python code and output are attached in a separate pdf document. However, I will still present the R code as it would have been here. 

```{r, eval=FALSE}

#the code originally did not work because there were columns that had all 0 responses
#so we had to remove them
raw.sum = apply(text_DTM_trimmed, 1, FUN = sum)
text_DTM_trimmed = text_DTM_trimmed[raw.sum!= 0,]


## write a loop to automatically output the perplexity
perp_by_col <- vector()
for (k in topics) {
  perp_by_row <- vector()
  for (chain in seq_len(10)) {
    training <- CTM(text_DTM_trimmed[folding != chain,], k = k,
                    control = control_CTM_VEM)
    testing <- CTM(text_DTM_trimmed[folding == chain,], model = training,
                   control = control_CTM_VEM)
    perp_by_row <- rbind(perp_by_row, perplexity(testing))
  }
  perp_by_col <- cbind(perp_by_col, perp_by_row)
}

 #wow this is indeed taking forever
```

```{r, eval = FALSE}
# Plot perplexity
transpose <- t(perp_by_col)
matplot(transpose, type = "l", col = rainbow(9), lty = 2, lwd = 2, ylab = "Perplexity", xlab = "K", main = "CTM-10-fold cross validation", xaxt="n")
axis(1, at=1:10, labels = c("k=2", "k=3", "k=4", "k=5", "k=6", "k=7", "k=8", "k=9", "k=10", "k=15"), cex=0.5)

perp_by_col_mean <- colMeans(perp_by_col)

lines(perp_by_col_mean, col = "black", lwd = 4, lty = 1)
led <- c("fold=2", "fold=3", "fold=4", "fold=5", "fold=6", "fold=7", "fold=8", "fold=9", "fold=10", "Average")
legend("topright", led, lwd = 2, lty = 2, col = c(rainbow(9), 'black'), cex = 0.65)

abline(v = 4, col = "gray60", lty = 2)
```

```{r, eval = FALSE}
{plot(perp_by_col_mean, pch = 20, ylab = 'Perplexity', xlab = "K", main = "CTM-10-fold cross validation", 
      xaxt = "n") 
  axis(1, at = 1:10, labels = c("k=2","k=3","k=4","k=5","k=6","k=7","k=8","k=9","k=10","k=15"), cex = 0.5)
  lines(perp_by_col_mean, lwd = 1, lty = 2, col = "red")}
```
Alright this is  10 topics

**Correlated Topic Model**
Running 10 correlated topic model
```{r, eval = FALSE}
control_CTM_VEM1 <- list(
  estimate.beta = TRUE, verbose=0, prefix=tempfile(),save=0,keep=0,
  seed=1421313709,nstart=1L,best=TRUE,
  var=list(iter.max=500,tol=10^-6),
  em=list(iter.max=1000,tol=10^-4),
  cg=list(iter.max=500,tol=10^5)
)

################## K = 10 ##################

CTM3 <- CTM(text_DTM_trimmed, k = 10, control = control_CTM_VEM1)
CTM3
```

**Examining the Topic Output**
```{r, eval = FALSE}

## Table the documents by topics
main_topic_table3 <- as.data.frame(table(topics(CTM3)))
colnames(main_topic_table3) <- c("Topic", "Frequency")
print(main_topic_table3)


#what are the top 10 words for the 3 topics
terms(CTM3,10)
```
```{r, eval = FALSE}
# Using this: https://www.tidytextmining.com/topicmodeling.html
# Use tidyverse to look at the CTM results a bit more more

tidy_topics <- tidy(CTM3, matrix = "beta")

top_terms <- tidy_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta) 

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_fill_brewer(palette = "Paired") +
  scale_y_reordered() +
  theme_bw()
```
```{r, eval = FALSE}

## Topics
topics3 <- posterior(CTM3)$topics
## Let's look at the probability of each document info fits into each of the topics
topics3 <- as.data.frame(topics3)
nrow(topics3)
rownames(topics3) <- posts1819$name
print(topics3)
## Let's look at which topic each document is assigned to one of the topics.
main_topic3 <- as.data.frame(topics(CTM3))
rownames(main_topic3) <- posts1819$doc_id
colnames(main_topic3) <- "Main_Topic"
print(main_topic3)
```

2 out of the 10 themes were posts removed for violating the Subreddit rules. The remaining 8 themes for 2018-2019 Reddit discussions related to #BLM and policing are:
1. Politics 
2. Gun Control
3. Police Brutality
4. Systemic Racism
5. Constructive Engagement
6. Government Control
7. Race & Crime
8. Mass Shootings

### 2020-2021 Reddit Posts
The process is similar to the 2018-2019 posts. The actual analyses in Python is attached in the separate pdf document. 

Similarly, 2 out of the 10 themes were posts removed for violating the Subreddit rules. The remaining 8 themes for 2020-2021 Reddit discussions related to #BLM and policing are:
1. Constructive Engagement 
2. Protests & riots
3. Destructive engagement
4. Election & Politics
5. Police Reforms
6. Rise of White Nationalists
7. Race & Crime
8. Gun Control

### Discussion

We can see that we are still discussing many of the same topics before the summer of 2020, which means that the conversation has not changed as much but rather the number of people engaging in discussions (i.e., politics, gun control, race and crime were present throughout the years). 

However, there were a few differences. For example, we saw a rise in destructive engagement in these spaces that was not the case in the year prior. For policing, we saw discussions shift from police brutality (bringing awareness to reality) to police reform (moving towards solution). In addition, in 2018-2019, conversations regarding race were mostly about systemic racism while in 2020-2021 we saw attention towards the rise of White nationalism. 

## Visualizing Topic Popularity Overtime

```{r data 18-19 import}
#this is the dataset with topics already assigned to them from python
reddit1819 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit1819tm.csv", na.string = "")
reddit1819$time_created <- anydate(reddit1819$time_created)

reddit1819$theme <- factor(reddit1819$Topic, levels = c(0,1,2,3,4,5,6,7,8,9) ,labels = c("Politics", "Gun Control", "Removed", "Police Brutality", "Systemic Racism", "Removed2", "Constructive Engagement", "Surveillance", "Race & Crime", "Mass Shooting"))

##assigning theme names
reddit1819 <- reddit1819 %>%
  dplyr::select(id, allcomment, time_created, title, post_flair, score, upvote_ratio, comms_num, body, author, author_flair, subreddit, alltext, manualtext, Topic, theme) %>%
  separate(time_created, into = c("year", "month", "date"), sep = "-")

reddit1819 %>%
  group_by(year) %>%
  count() #not sure why 2019 is being shown as 1969 and 1970
#i double checked to see whether there was actually no 2019 data, but it was definitely 2019 data 
#with the original data set, the years showed as 2019

#manually reassign values for 2019
reddit1819[year == "1969",]$year <- 2019
reddit1819[year == "1970",]$year <- 2019

```


```{r data 20-21 import}
#this is the dataset with topics already assigned to them from python
reddit2021 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2021tm.csv", na.string = "")
reddit2021$time_created <- anydate(reddit2021$time_created)

reddit2021$theme <- factor(reddit2021$Topic, levels = c(0,1,2,3,4,5,6,7,8,9) ,labels = c("Constructive Engagement", "Removed", "Protest & Riot", "Destructive Engagement", "Politics", "Removed2", "Police Reform", "White Nationalism", "Race & Crime", "Gun Control"))

##assigning 
reddit2021 <- reddit2021 %>%
  dplyr::select(id, allcomment, time_created, title, post_flair, score, upvote_ratio, comms_num, body, author, author_flair, subreddit, alltext, manualtext, Topic, theme) %>%
  separate(time_created, into = c("year", "month", "date"), sep = "-")

#all topics
reddit_tm <- bind_rows(reddit1819, reddit2021)

reddit_tm <- reddit_tm %>%
  unite(time_created, year, month, date, sep = "-", remove = FALSE) 
reddit_tm$time_created <- ymd(reddit_tm$time_created) #convert to date variable

#this will make visualization looks prettier later by ordering factor
reddit_tm$theme <- factor(reddit_tm$theme, levels = c("Race & Crime", "Police Brutality", "Police Reform", "Constructive Engagement","Systemic Racism", "Destructive Engagement", "Gun Control", "Mass Shooting", "Protest & Riot", "Politics", "Surveillance", "White Nationalism"))
  
```

```{r same 4 topic time}
reddit_tm %>%
  filter(theme %in% c("Politics", "Gun Control", "Constructive Engagement", "Race & Crime"))%>%
  group_by(year, month, theme) %>%
  mutate(count = n()) %>%
  ggplot(aes(x = time_created)) +
  geom_line(aes(y = count)) +
  facet_wrap(~theme, ncol = 1, strip.position = "bottom") +
  theme_minimal() +
  labs(title = "Evolution of Four Topics, from 2018-2021") +
  theme(
    # Light background color
    plot.background = element_rect(fill = "#F5F4EF", color = NA),
    plot.margin = margin(20, 30, 20, 30),
    # Customize the title. Note the new font family and its larger size.
    plot.title = element_text(
      margin = margin(0, 0, 10, 0), 
      size = 12, 
      face = "bold", 
      vjust = 0, 
      color = "grey25"
    ),
    plot.caption = element_text(size = 11),
    # Remove titles for x and y axes.
    axis.title = element_blank(),
    # Specify color for the tick labels along both axes 
    axis.text = element_text(color = "grey40", size = 6),
    # Specify face and color for the text on top of each panel/facet
    strip.text = element_text(face = "bold", color = "grey20")
  )
```

```{r all topic over time}
reddit_tm %>%
  filter(!(theme == "Removed" | theme == "Removed2")) %>%
  group_by(year, month, theme) %>%
  mutate(count = n()) %>%
  ggplot(aes(x = time_created)) +
  geom_line(aes(y = count)) +
  facet_wrap(~theme, ncol = 3, strip.position = "bottom") +
  theme_minimal() +
  labs(title = "Evolution of All Topics, from 2018-2021") +
  theme(
    # Light background color
    plot.background = element_rect(fill = "#F5F4EF", color = NA),
    plot.margin = margin(20, 30, 20, 30),
    # Customize the title. Note the new font family and its larger size.
    plot.title = element_text(
      margin = margin(0, 0, 10, 0), 
      size = 12, 
      face = "bold", 
      vjust = 0, 
      color = "grey25"
    ),
    plot.caption = element_text(size = 11),
    # Remove titles for x and y axes.
    axis.title = element_blank(),
    # Specify color for the tick labels along both axes 
    axis.text = element_text(color = "grey40", size = 6),
    # Specify face and color for the text on top of each panel/facet
    strip.text = element_text(face = "bold", color = "grey20")
  )

```


## Sentiment Analysis
Sentiment analysis codes were adapted from 
Silge, J., Robinson, D. (2017). Text Mining with R: A Tidy Approach. O'Reilly.
```{r packages for sentiment}
library(tidytext)
library(textdata)
nrc_di <- get_sentiments("nrc") %>%
  filter(sentiment %in% c("positive", "negative"))
```

### 2018-2019 Reddit Posts

```{r tidy 18-19 w sentiment}
tidy_r1819 <- reddit1819 %>%
  filter(!(Topic == 2 | Topic == 5)) %>%
  unnest_tokens(word, manualtext)
```

```{r sentiment afinn plot}

tidy_r1819 %>%
  inner_join(get_sentiments("afinn")) %>%
  unite(ym, year, month, sep = "-") %>%
  arrange(ym, .by_group = TRUE) %>%
  summarise(sentiment = sum(value),
            theme = theme,
            index = ym) %>%
  ggplot(aes(index, sentiment, fill = theme)) +
  geom_col(show.legend = FALSE) +
  xlab("")+
  facet_wrap(~theme, ncol = 4) +
  scale_fill_brewer(palette = "Paired") +
  scale_x_discrete(breaks = c("2018-01", "2019-01", "2020-01", "2021-01"), labels = c("2018", "2019", "2020", "2021")) +
  theme_minimal() +
  labs(title = "'afinn' Topic Sentiment Over Time, 2018-2019",
       caption = "Values below 0 indicate more negative sentiment") +
  theme(
    # Light background color
    plot.background = element_rect(fill = "#F5F4EF", color = NA),
    plot.margin = margin(20, 30, 20, 30),
    # Customize the title. Note the new font family and its larger size.
    plot.title = element_text(
      margin = margin(0, 0, 10, 0), 
      size = 12, 
      face = "bold", 
      vjust = 0, 
      color = "grey25"
    ),
    plot.caption = element_text(size = 6),
    # Remove titles for x and y axes.
    axis.title = element_blank(),
    # Specify color for the tick labels along both axes 
    axis.text = element_text(color = "grey40", size = 6),
    # Specify face and color for the text on top of each panel/facet
    strip.text = element_text(face = "bold", color = "grey20", size = 8)
  )

```
```{r}
tidy_r1819 %>%
  inner_join(nrc_di) %>%
  unite(ym, year, month, sep = "-") %>%
  arrange(ym, .by_group = TRUE) %>%
  count(theme, index = ym, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative) %>%
  ggplot(aes(index, sentiment, fill = theme)) +
  geom_col(show.legend = FALSE) +
  xlab("")+
  scale_x_discrete(breaks = NULL, labels = NULL) +
  scale_fill_brewer(palette = "Paired") +
  scale_x_discrete(breaks = c("2018-01", "2019-01", "2020-01", "2021-01"), labels = c("2018", "2019", "2020", "2021")) +
  facet_wrap(~theme, ncol = 4) +
  theme_minimal() +
  labs(title = "'nrc' Topic Sentiment Over Time, 2018-2019", 
       caption = "Values below 0 indicate more negative sentiment") +
  theme(
    # Light background color
    plot.background = element_rect(fill = "#F5F4EF", color = NA),
    plot.margin = margin(20, 30, 20, 30),
    # Customize the title. Note the new font family and its larger size.
    plot.title = element_text(
      margin = margin(0, 0, 10, 0), 
      size = 12, 
      face = "bold", 
      vjust = 0, 
      color = "grey25"
    ),
    plot.caption = element_text(size = 6),
    # Remove titles for x and y axes.
    axis.title = element_blank(),
    # Specify color for the tick labels along both axes 
    axis.text = element_text(color = "grey40", size = 6),
    # Specify face and color for the text on top of each panel/facet
    strip.text = element_text(face = "bold", color = "grey20", size = 8)
  )

```
Shows similar direction to afinn; afinn has more variance but less positive
### 2020-2021 Reddit Posts

```{r tidy 20-21 data}
tidy_r2021 <- reddit2021 %>%
  filter(!(Topic == 1 | Topic == 5)) %>%
  unnest_tokens(word, manualtext)
```


```{r sentiment 20-21}
tidy_r2021 %>%
  inner_join(nrc_di) %>%
  unite(ym, year, month, sep = "-") %>%
  arrange(ym, .by_group = TRUE) %>%
  count(theme, index = ym, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative) %>%
  ggplot(aes(index, sentiment, fill = theme)) +
  geom_col(show.legend = FALSE) +
  xlab("")+
  scale_x_discrete(breaks = c("2018-01", "2019-01", "2020-01", "2021-01"), labels = c("2018", "2019", "2020", "2021")) +
  scale_fill_brewer(palette = "Paired") +
  facet_wrap(~theme, ncol = 4) +
  theme_minimal() +
  labs(title = "'nrc' Topic Sentiment Over Time, 2020-2021", 
       caption = "Values below 0 indicate more negative sentiment") +
  theme(
    # Light background color
    plot.background = element_rect(fill = "#F5F4EF", color = NA),
    plot.margin = margin(20, 30, 20, 30),
    # Customize the title. Note the new font family and its larger size.
    plot.title = element_text(
      margin = margin(0, 0, 10, 0), 
      size = 12, 
      face = "bold", 
      vjust = 0, 
      color = "grey25"
    ),
    plot.caption = element_text(size = 6),
    # Remove titles for x and y axes.
    axis.title = element_blank(),
    # Specify color for the tick labels along both axes 
    axis.text = element_text(color = "grey40", size = 6),
    # Specify face and color for the text on top of each panel/facet
    strip.text = element_text(face = "bold", color = "grey20", size = 8)
  )
```

### Across All Data
```{r tidy 4 topics sentiment}
tidy_r1821<- reddit_tm %>%
  filter(theme %in% c("Politics", "Gun Control", "Constructive Engagement", "Race & Crime"))%>%
  unnest_tokens(word, manualtext)

tidy_r1821 %>%
  inner_join(nrc_di) %>%
  unite(ym, year, month, sep = "-") %>%
  arrange(ym, .by_group = TRUE) %>%
  count(theme, index = ym, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative) %>%
  ggplot(aes(index, sentiment, fill = theme)) +
  geom_col(show.legend = FALSE) +
  xlab("")+
  scale_fill_brewer(palette = "Paired") +
  scale_x_discrete(breaks = c("2018-01", "2019-01", "2020-01", "2021-01"), labels = c("2018", "2019", "2020", "2021")) +
  facet_wrap(~theme, ncol = 1, strip.position = "bottom") +
  theme_minimal() +
  labs(title = "Topic Sentiment Over Time, from 2018-2021") +
  theme(
    # Light background color
    plot.background = element_rect(fill = "#F5F4EF", color = NA),
    plot.margin = margin(20, 30, 20, 30),
    # Customize the title. Note the new font family and its larger size.
    plot.title = element_text(
      margin = margin(0, 0, 10, 0), 
      size = 12, 
      face = "bold", 
      vjust = 0, 
      color = "grey25"
    ),
    plot.caption = element_text(size = 11),
    # Remove titles for x and y axes.
    axis.title = element_blank(),
    # Specify color for the tick labels along both axes 
    axis.text = element_text(color = "grey40", size = 6),
    # Specify face and color for the text on top of each panel/facet
    strip.text = element_text(face = "bold", color = "grey20")
  )

```

## Lexical Diversity 
Lexical Diversity from Quanteda package: https://tutorials.quanteda.io/statistical-analysis/lexdiv/
Lexical diversity measures number of unique types of tokens and the length of a document. It is useful for analysing speakers' or writers' linguistic skills, or the complexity of ideas expressed in documents
Also codes from: https://towardsdatascience.com/lyrics-analysis-5e1990070a4b
```{r}
library(quanteda)
library(quanteda.textstats)

reddit_4tm <- reddit_tm 
  

corpus_1821 <- corpus(reddit_4tm, text_field = "manualtext")
#get number of tokens in each comments
docvars(corpus_1821)$ntoken <- corpus_1821 %>%
  ntoken()
#filter out shorter posts and comments
corpus_1821 <- corpus_subset(corpus_1821, ntoken > 10)

  
toks_1821 <- tokens(corpus_1821)

dfmat_1821 <- dfm(toks_1821, remove = stopwords("en"))

tstat_lexdiv <- textstat_lexdiv(dfmat_1821, remove_numbers = TRUE, remove_hyphens = TRUE)

#merge document back
lexdiv_1821 <- bind_cols(docvars(corpus_1821), tstat_lexdiv[,1:2])


#order factor levels so that visualization will look more pretty later
lexdiv_1821$theme <- factor(lexdiv_1821$theme, levels = c("Race & Crime", "Police Brutality", "Police Reform", "Constructive Engagement","Systemic Racism", "Destructive Engagement", "Gun Control", "Mass Shooting", "Protest & Riot", "Politics", "Surveillance", "White Nationalism"))


colnames(lexdiv_1821)
```

```{r}
lexdiv_1821 %>%
  filter(theme %in% c("Politics", "Gun Control", "Constructive Engagement", "Race & Crime"))%>%
  group_by(year, month, theme) %>%
  mutate(lexdiv = mean(TTR)) %>%
  ggplot(aes(x = time_created)) +
  geom_line(aes(y = lexdiv)) +
  facet_wrap(~theme, ncol = 2, strip.position = "bottom") +
  theme_minimal() +
  labs(title = "Lexical Diversity Over Time, from 2018-2021") +
  theme(
    # Light background color
    plot.background = element_rect(fill = "#F5F4EF", color = NA),
    plot.margin = margin(20, 30, 20, 30),
    # Customize the title. Note the new font family and its larger size.
    plot.title = element_text(
      margin = margin(0, 0, 10, 0), 
      size = 12, 
      face = "bold", 
      vjust = 0, 
      color = "grey25"
    ),
    plot.caption = element_text(size = 11),
    # Remove titles for x and y axes.
    axis.title = element_blank(),
    # Specify color for the tick labels along both axes 
    axis.text = element_text(color = "grey40", size = 6),
    # Specify face and color for the text on top of each panel/facet
    strip.text = element_text(face = "bold", color = "grey20")
  )


```


```{r lexdiv graph}
lexdiv_1821 %>%
  filter(!(theme == "Removed" | theme == "Removed2")) %>%
  group_by(year, month, theme) %>%
  mutate(lexdiv = mean(TTR)) %>%
  ggplot(aes(x = time_created)) +
  geom_line(aes(y = lexdiv)) +
  facet_wrap(~theme, ncol = 3, strip.position = "bottom") +
  theme_minimal() +
  labs(title = "Lexical Diversity for All Topics, from 2018-2021") +
  theme(
    # Light background color
    plot.background = element_rect(fill = "#F5F4EF", color = NA),
    plot.margin = margin(20, 30, 20, 30),
    # Customize the title. Note the new font family and its larger size.
    plot.title = element_text(
      margin = margin(0, 0, 10, 0), 
      size = 12, 
      face = "bold", 
      vjust = 0, 
      color = "grey25"
    ),
    plot.caption = element_text(size = 11),
    # Remove titles for x and y axes.
    axis.title = element_blank(),
    # Specify color for the tick labels along both axes 
    axis.text = element_text(color = "grey40", size = 6),
    # Specify face and color for the text on top of each panel/facet
    strip.text = element_text(face = "bold", color = "grey20")
  )

```


## Correlations: Sentiment and Engagement
What if we study the posts with higher engagement vs. lower engagement 
Run correlations?
```{r}
nrc_small <- get_sentiments("nrc") %>%
  filter(sentiment %in% c("trust", "anticipation", "joy", 
                        "fear", "anger", "disgust", "sadness"))
reddit1819raw <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/clean1819raw.csv", na.string = "")
reddit1819raw$time_created <- anydate(reddit1819raw$time_created)
colnames(reddit1819raw)
reddit1819raw <- reddit1819raw %>%
  dplyr::select(id, time_created, finaltitle, post_flair, score, upvote_ratio, comms_num, title, body, author, author_flair, comment_id, comment_parent_id, finalcomment, comment_score, comment_author, subreddit)

#just post titles
res1819raw <- reddit1819raw %>%
  group_by(id) %>%
  unnest_tokens(word, finaltitle) %>%
  inner_join(nrc_small) %>%
  group_by(sentiment)# %>%
  #mutate(count = n())

setDT(res1819raw)
#make the frame longer so that each sentiment is a column 
res1819raw <- dcast.data.table(res1819raw, id~sentiment, value.var= 'sentiment')

res1819raw <- res1819raw %>%
  inner_join(reddit1819raw, by = c("id" = "id"))

colnames(res1819raw)
res1819corr <- res1819raw %>%
  dplyr::select(comms_num, score, upvote_ratio, joy, trust, anticipation, anger, disgust, fear, sadness)

correlation_matrix <- round(cor(res1819corr),1)
corrp.mat <- cor_pmat(res1819corr) #calculating p-vals

ggcorrplot(correlation_matrix, hc.order =TRUE, 
           type ="upper", p.mat = corrp.mat, insig="blank") +
  theme_minimal() +
  labs(title = "Correlation: Engagement and Sentiment, 2018-2019") +
  theme(
    # Elements within a guide are placed one next to the other in the same row
    legend.direction = "vertical",
    # No legend title
    legend.title = element_blank(),
    legend.text = element_text(color = "grey20", size = 8),
    # Light background color
    plot.background = element_rect(fill = "#F5F4EF", color = NA),
    plot.margin = margin(20, 30, 20, 30),
    # Customize the title. Note the new font family and its larger size.
    plot.title = element_text(
      margin = margin(0, 0, 10, 0), 
      size = 11, 
      face = "bold", 
      vjust = 0, 
      color = "grey25"
    ),
    plot.caption = element_text(size = 6),
    # Remove titles for x and y axes.
    axis.title = element_blank(),
    # Specify color for the tick labels along both axes 
    axis.text = element_text(color = "grey40", size = 6),
    # Specify face and color for the text on top of each panel/facet
    strip.text = element_text(face = "bold", color = "grey20", size = 8)
  )

```
Fear, sadness, anger, disgust, anticipation, and trust sentiments in the title are positively correlated with number of comments and score. Joy is negatively correlated with comment numbers. 

```{r}
#for 20-21
reddit2021raw <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/clean2021raw.csv", na.string = "")
reddit2021raw$time_created <- anydate(reddit2021raw$time_created)
colnames(reddit2021raw)
reddit2021raw <- reddit2021raw %>%
  dplyr::select(id, time_created, finaltitle, post_flair, score, upvote_ratio, comms_num, title, body, author, author_flair, comment_id, comment_parent_id, finalcomment, comment_score, comment_author, subreddit)

#just post titles
res2021raw <- reddit2021raw %>%
  group_by(id) %>%
  unnest_tokens(word, finaltitle) %>%
  inner_join(nrc_small) %>%
  group_by(sentiment)# %>%
  #mutate(count = n())

setDT(res2021raw)
#make the frame longer so that each sentiment is a column 
res2021raw <- dcast.data.table(res2021raw, id~sentiment, value.var= 'sentiment')

res2021raw <- res2021raw %>%
  inner_join(reddit2021raw, by = c("id" = "id"))

colnames(res2021raw)
res2021corr <- res2021raw %>%
  dplyr::select(comms_num, score, upvote_ratio, joy, trust, anticipation, anger, disgust, fear, sadness)

correlation_matrix <- round(cor(res2021corr),1)
corrp.mat <- cor_pmat(res2021corr) #calculating p-vals

ggcorrplot(correlation_matrix, hc.order =TRUE, 
           type ="upper", p.mat = corrp.mat, insig="blank") +
  theme_minimal() +
  labs(title = "Correlation: Engagement and Sentiment, 2018-2019") +
  theme(
    # Elements within a guide are placed one next to the other in the same row
    legend.direction = "vertical",
    # No legend title
    legend.title = element_blank(),
    legend.text = element_text(color = "grey20", size = 8),
    # Light background color
    plot.background = element_rect(fill = "#F5F4EF", color = NA),
    plot.margin = margin(20, 30, 20, 30),
    # Customize the title. Note the new font family and its larger size.
    plot.title = element_text(
      margin = margin(0, 0, 10, 0), 
      size = 11, 
      face = "bold", 
      vjust = 0, 
      color = "grey25"
    ),
    plot.caption = element_text(size = 6),
    # Remove titles for x and y axes.
    axis.title = element_blank(),
    # Specify color for the tick labels along both axes 
    axis.text = element_text(color = "grey40", size = 6),
    # Specify face and color for the text on top of each panel/facet
    strip.text = element_text(face = "bold", color = "grey20", size = 8)
  )


```

It seems that unlike 2018-2019, 2020 and 2021 posts with more positive sentiments get higher engagement while disgust had no significant correlation with engagement. 

**[End of Document]**




