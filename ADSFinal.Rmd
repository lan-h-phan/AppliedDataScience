---
title: "Applied Data Science Final"
author: "Lan Phan"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: TRUE
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)

library(dplyr)
library(tidyverse)
library(ggplot2)
library(readxl)
library(data.table)
library(RColorBrewer)
```

# Tidy Data

A series of steps was taken to compile, clean, and sort the various data sets used in this paper. The attached markdown document reflects the initial process. 

## Import Datasets

As some datasets used full state names while others only had abbreviations, I made an excel spreadsheet called statetranslation in order to connect different datasets based on states. The statetranslation document only had 2 variables, one for full state names, the other is the abbreviation. 

```{r}
statetranslation <- read_xlsx("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/StateTranslation.xlsx")

#US Census Data 2020
censusdat <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/censusclean2020.csv")
censusdat <- censusdat %>%
  mutate(rWhite = White/ PopEstimate,
         rBlack = Black/ PopEstimate,
         rAsian = Asian/ PopEstimate,
         rNative = Native/ PopEstimate,
         rOther = Other/ PopEstimate,
         rMixed = `Inter-racial`/ PopEstimate)

#police violence data
policeviolence <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/policeviolence2018-2021.csv")


# hatecrime count from 2018-2020
hatecrime <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/hatecrime2018-2020.csv")

#tweet count related to BLM 
tweetcount <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/blmtweetcount2018-2021.csv")

#demonstrations count 2018
c3compiled <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/c3clean.csv")

c3allstate <- read_csv("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/c3clean.csv")
c3allstate <- c3allstate %>%
  mutate(type = if_else(valence %in% c(0,1), 1, 2)) %>% #code if the protest is for or against BLM
  count(state, year, type) %>%
  left_join(statetranslation,
            by = c("state" = "Abbrev")) %>%
  filter(!is.na(state))

c3allstate$Name <- as.factor(c3allstate$Name)

c3allstate <- c3allstate %>%
  filter(year %in% c(2018,2019,2020,2021,2022)) %>%
  pivot_wider(names_from = type, values_from = n)

c3allstate$`2`[is.na(c3allstate$`2`)] <- 1

```

# Data Visualization

## Descriptive Stats

## Heatmap

```{r}
#packages
library(ComplexHeatmap)
library(hopach) 
library(circlize)
```

```{r}

#function uncentered correlation and average linkage
uncenter.dist<-function(m) {
  as.dist(as.matrix(distancematrix(m, d="cosangle")))
}


best_col = colorRamp2(c(-2, 0, 2), c("#82A3FF", "grey", "#8B2E2E"))


#Setting up= for the 2018 heatmap

#counting hate crime in 2018
hcsum2018 <- hatecrime %>%
  filter(DATA_YEAR == 2018) %>%
  group_by(STATE_ABBR) %>%
  summarize(HateCrime = n())

pvsum2018 <- policeviolence %>%
  filter(year == 2018) %>%
  group_by(state) %>%
  summarize(PoliceViolence = n())


datsum2018 <- censusdat %>%
  left_join(statetranslation,
             by = c("NAME" = "Name"))

datsum2018 <- datsum2018 %>%
  left_join(hcsum2018,
            by = c("Abbrev" = "STATE_ABBR")) %>%
  left_join(pvsum2018,
            by = c("Abbrev" = "state")) %>%
  mutate(rHateCrime = (HateCrime/ PopEstimate)*100,
         rPoliceViolence = (PoliceViolence/ PopEstimate)*100) %>%
  dplyr::select(NAME,
         rWhite, rBlack, rNative, rAsian, rOther, rMixed,
         rPoliceViolence, rHateCrime)

hm2018dat<-datsum2018[, c(2:9)]

#name the heatmap rows to their states
rownames(hm2018dat) <- datsum2018$NAME

#z-scoring the data
hm2018dat.scale<-scale(hm2018dat)

#uncentered correlation and average linkage

row.clus18<-hclust(uncenter.dist(hm2018dat.scale), method = "ave")
col.clus18<-hclust(uncenter.dist(t(hm2018dat.scale)), method = "ave")

ht2018 = Heatmap(hm2018dat.scale, name = "2018 Heatmap", 
        cluster_rows=row.clus18, cluster_columns=col.clus18, 
        na_col = "white", col = best_col,
        row_names_gp = gpar(fontsize = 5))

```

```{r}
#2019 Heatmap

#Setting up for the 2019 heatmap

hcsum2019 <- hatecrime %>%
  filter(DATA_YEAR == 2019) %>%
  group_by(STATE_ABBR) %>%
  summarize(HateCrime = n())

pvsum2019 <- policeviolence %>%
  filter(year == 2019) %>%
  group_by(state) %>%
  summarize(PoliceViolence = n())


datsum2019 <- censusdat %>%
  left_join(statetranslation,
             by = c("NAME" = "Name"))

datsum2019 <- datsum2019 %>%
  left_join(hcsum2019,
            by = c("Abbrev" = "STATE_ABBR")) %>%
  left_join(pvsum2019,
            by = c("Abbrev" = "state")) %>%
  mutate(rHateCrime = (HateCrime/ PopEstimate)*100,
         rPoliceViolence = (PoliceViolence/ PopEstimate)*100) %>%
  dplyr::select(NAME,
         rWhite, rBlack, rNative, rAsian, rOther, rMixed,
         rPoliceViolence, rHateCrime)

hm2019dat<-datsum2019[, c(2:9)]

#name the heatmap rows to their states
rownames(hm2019dat) <- datsum2019$NAME

#z-scoring the data
hm2019dat.scale<-scale(hm2019dat)

#uncentered correlation and average linkage

row.clus19<-hclust(uncenter.dist(hm2019dat.scale), method = "ave")
col.clus19<-hclust(uncenter.dist(t(hm2019dat.scale)), method = "ave")

ht2019 = Heatmap(hm2019dat.scale, name = "2019 Heatmap", 
        cluster_rows=row.clus19, cluster_columns=col.clus19, 
        na_col = "white", col = best_col,
        row_names_gp = gpar(fontsize = 5))
```

```{r}
#2020 Heatmap


hcsum2020 <- hatecrime %>%
  filter(DATA_YEAR == 2020) %>%
  group_by(STATE_ABBR) %>%
  summarize(HateCrime = n())

pvsum2020 <- policeviolence %>%
  filter(year == 2020) %>%
  group_by(state) %>%
  summarize(PoliceViolence = n())


datsum2020 <- censusdat %>%
  left_join(statetranslation,
             by = c("NAME" = "Name"))

datsum2020 <- datsum2020 %>%
  left_join(hcsum2020,
            by = c("Abbrev" = "STATE_ABBR")) %>%
  left_join(pvsum2020,
            by = c("Abbrev" = "state")) %>%
  mutate(rHateCrime = (HateCrime/ PopEstimate)*100,
         rPoliceViolence = (PoliceViolence/ PopEstimate)*100) %>%
  dplyr::select(NAME,
         rWhite, rBlack, rNative, rAsian, rOther, rMixed,
         rPoliceViolence, rHateCrime)

hm2020dat<-datsum2020[, c(2:9)]

#name the heatmap rows to their states
rownames(hm2020dat) <- datsum2020$NAME

#z-scoring the data
hm2020dat.scale<-scale(hm2020dat)

#uncentered correlation and average linkage

row.clus20<-hclust(uncenter.dist(hm2020dat.scale), method = "ave")
col.clus20<-hclust(uncenter.dist(t(hm2020dat.scale)), method = "ave")

ht2020 = Heatmap(hm2020dat.scale, name = "2020 Heatmap", 
        cluster_rows=row.clus20, cluster_columns=col.clus20, 
        na_col = "white", col = best_col,
        row_names_gp = gpar(fontsize = 5))
```

```{r}

ht2018 + ht2019 + ht2020
```

## Heatmap Maps! 

(officially known as Choropleth)

Parts of the codes are from: https://rdvark.net/2021/12/29/pretty-choropleth-maps-with-sf-and-ggplot2/

For 2018
```{r}
library(leaflet)
library(sf)

#importing the shape of the U.S.
usmap <- read_sf("/Users/lanphan/Desktop/Academics/PhD Research/GitHub-R-Codes/AppliedDataScience/gz_2010_us_040_00_5m.json")

ccc2018 <- c3compiled %>%
  filter(year == 2018)

choroplot18 <- ccc2018 %>%
  group_by(state) %>%
  summarize(count2018 = n()) %>%
  right_join(statetranslation,
             by = c("state" = "Abbrev")) %>%
  inner_join(censusdat,
             by = c("Name" = "NAME")) %>%
  mutate(ratio2018 = (count2018/PopEstimate)*100) %>%
  inner_join(usmap,
             by = c("Name" = "NAME"))

#turn back into a sf object for geom_sf to map out data
choroplot18 <- st_as_sf(choroplot18)


#### Just by count
choroplot18 %>% 
  ggplot(aes(fill = count2018)) + # create a ggplot object and 
  # change its fill colour according to median_age
  geom_sf(colour = NA) +# plot all local authority geometries 
  scale_fill_gradient(low = "#82A3FF", 
                      high = "#8B2E2E", na.value = "white") +
  coord_sf(xlim = c(-180, -60), lims_method = "box") + #just zoom in the U.S.
  theme_void() #remove background

#### By ratio of the population
choroplot18 %>% 
  ggplot(aes(fill = ratio2018)) + # create a ggplot object and 
  # change its fill colour according to median_age
  geom_sf(colour = NA) +# plot all local authority geometries 
  scale_fill_gradient(low = "#82A3FF", 
                      high = "#8B2E2E", na.value = "white") +
  coord_sf(xlim = c(-180, -60), lims_method = "box") + #just zoom in the U.S.
  theme_void() #remove background

#finally omg
#so beautiful imma cry
#also work with plotly
```

## Changes in Demonstrations by Year and States

```{r}

###Font Settings
library(showtext)
library(Hmisc)

font_add_google("Fira Sans", family = "firasans")
font_add_google("Lato", family = "lato")


stategeofacet <- c3allstate %>%
  ggplot(aes(x = year, group = 1)) +
  geom_line(aes(y = `2`, color = "counter-protest")) +
  geom_line(aes(y = `1`, color = "for BLM")) +
  stat_difference(aes(ymin = `1`, ymax = `2`), alpha = .3) +
  facet_geo(vars(state), grid = "us_state_grid1")

stategeofacet <- stategeofacet +
  # Colors for the lines
  scale_color_manual(values = c("#3D85F7", "#C32E5A")) +
  scale_fill_manual(
    values = c(
      colorspace::lighten("#3D85F7"), 
      colorspace::lighten("#C32E5A"),
      "grey60"),
    labels = c("more BLM", "more counter-protest")
  ) +
  # Add labels
  labs(
    title = "Number of demonstrations \nfor and against #BlackLivesMatter \nfrom 2018-2021",
    caption = "Source: CrowdCountingConsortium"
  ) +
  # Specify the order for the guides
  guides(
    # Order indicates the order of each legend among multiple guides.
    # The guide for 'color' will be placed before the onde for 'fill'
    color = guide_legend(order = 1), 
    fill = guide_legend(order = 2)
  ) 



stategeofacet <- stategeofacet +
  # A minimalistic theme with no background annotations
  theme_minimal() +
  theme(
    # Top-right position
    legend.pos = c(0.875, 0.2),
    # Elements within a guide are placed one next to the other in the same row
    legend.direction = "horizontal",
    # Different guides are stacked vertically
    legend.box = "vertical",
    # No legend title
    legend.title = element_blank(),
    # Light background color
    plot.background = element_rect(fill = "#F5F4EF", color = NA),
    plot.margin = margin(20, 30, 20, 30),
    # Customize the title. Note the new font family and its larger size.
    plot.title = element_text(
      margin = margin(0, 0, -50, 0), 
      size = 14, 
      face = "bold", 
      vjust = 0, 
      color = "grey25"
    ),
    plot.caption = element_text(size = 11),
    # Remove titles for x and y axes.
    axis.title = element_blank(),
    # Specify color for the tick labels along both axes 
    axis.text = element_text(color = "grey40", size = 6),
    # Specify face and color for the text on top of each panel/facet
    strip.text = element_text(face = "bold", color = "grey20")
  )

stategeofacet
```


# Data Analysis

## Topic Modeling 

The codes are from 

Bowers, A.J., Chen, J.(2015) Ask and Ye Shall Receive? Automated Text Mining of Michigan Capital Facility Finance Bond Election Proposals to Identify which Topics are Associated with Bond Passage and Voter Turnout. Journal of Education Finance, 41(2), p.164-196. http://dx.doi.org/10.7916/D8FJ2GDH

```{r}
library(topicmodels)
library(tm)
library(tidytext)
library(tidyr)
library(slam)
library(ggrepel)
library(MASS)
library(textstem)
library(readtext)
```

### 2018 Reddit Posts
```{r}
#package to change the time into something that makes sense
library(anytime)

#Importing Reddit Data for 2018
comments2018 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2018comments.csv", na.string = "")

posts2018 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2018posts.csv", na.string = "")

#reshaping 2018 data by matching post and comments together based on the post_id 
all2018 <- posts2018[comments2018, on = c(id = "submission")] #right_joining 

all2018$time_created <- anydate(all2018$time_created)

all2018 <- all2018 %>%
  separate(time_created, into = c("year", "month", "date"), sep = "-", remove = FALSE) 

all2018 <- all2018[year == 2018]


#Divide up into subreddits


#Discussion-based
#politics, politicaldiscussion, and neutralpolitics
discussion2018 <- all2018[subreddit %in% c("politics", "politicaldiscussion", "neutralpolitics")]
discussion2018p <- posts2018[subreddit %in% c("politics", "politicaldiscussion", "neutralpolitics")] #just post titles


#opinion-based
# changemyview, politicalopinions
opinion2018 <- all2018[subreddit %in% c("changemyview", "politicalopinions")]
opinion2018p <- posts2018[subreddit %in% c("changemyview", "politicalopinions")] #just post titles



#query-based
#asktrumpsupporters and ask_politics
query2018 <- all2018[subreddit %in% c("asktrumpsupporters", "ask_politics")]
#none in 2018

posts2019 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2019posts.csv", na.string = "")

posts1819 <- rbind(posts2018, posts2019)
```

```{r}
posttext1819 <- posts1819 %>%
  unnest_tokens(word, title) %>%
  anti_join(stop_words)

posttext1819 %>%
  group_by(word) %>%
  count()
  
```


```{r}
#random select 100 rows cause it is too much 
#ctmdat <- sample_n(discussion2018p, 200)
ctmdat <- posts1819
```

*Learning Notes from Dr. Bowers*
topicmodels and tm needs a very specific type of data, called a DocumentTermMatrix. We’ll call specific corpus and DocumentTermMatrix functions here to create the data matrix required for the topic model.

Note here that this is where we stem, stopword, set the min word length, remove numbers, and remove punctuation.

So this sets up our data matrix DTM. We then need to know how to set “alpha” the hyperparameter, so we need some information on the tfidf (term frequency-inverse document frequency)

```{r}
#Build CMT
corpus <- Corpus(VectorSource(ctmdat$title))

#text manipulation
text_DTM <- DocumentTermMatrix(corpus, control = list(stemming = TRUE, 
                                                      stopwords = TRUE, stopwords = c("starbucks", "netanyahu", "beto", "didnt", "shouldnt", "illinois", "michigan", "metoo", "israel", "aurora", "kavanaugh", "trump", "blm", "blacklivesmatter"), minWordLength = 3, 
                                                      removeNumbers = TRUE, removePunctuation = TRUE))

term_tfidf <- tapply(text_DTM$v / row_sums(text_DTM)[text_DTM$i], text_DTM$j, mean) * log2(nDocs(text_DTM)/col_sums(text_DTM > 0))
summary(term_tfidf)

plot(density(term_tfidf))
                            
```
Rule of thumb is to use the median from the graph for alpha. Here it's around 0.7.
We will cut down the total number of terms by setting alpha so that we don't have too many common terms no unique ones. 
```{r}
alpha <- 0.90 #is the median
text_DTM_trimmed <- text_DTM[row_sums(text_DTM) >0, term_tfidf >= alpha]
dim(text_DTM_trimmed)
```

**10-fold Cross Validation**

We set up the 10 fold cross validation, selecting 10% of the data for each of the 10 folds

```{r}
# Cross validation

control_CTM_VEM <- list(
  estimate.beta = TRUE, verbose = 0, prefix = tempfile(), save = 0, keep = 0,
  seed = as.integer(Sys.time()), nstart=1L, best = TRUE,
  var = list(iter.max=100, tol=10^-6),
  em = list(iter.max=500, tol=10^-4),
  cg = list(iter.max=100, tol=10^5)
)

# use 10-fold CV to determine k!
# randomly divide the data into 10 folds.

set.seed(123)
topics <- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15) ##set k equals to 2 3 4 5 6 7 8 9 10 15.
seed <- 2
D <- length(ctmdat$title) 
folding <- sample(rep(seq_len(10), ceiling(D))[seq_len(D)])
table(folding)
```

**Learning Notes**
We train and test using the 10-fold cross validation that we just set up. Then plot by perplexity to see where the elbow is to select the correct number of topics k

Note: This can take a bit of time.

```{r}


#the code originally did not work because there were columns that had all 0 responses
#so we had to remove them
raw.sum = apply(text_DTM_trimmed, 1, FUN = sum)
text_DTM_trimmed = text_DTM_trimmed[raw.sum!= 0,]


## write a loop to automatically output the perplexity
perp_by_col <- vector()
for (k in topics) {
  perp_by_row <- vector()
  for (chain in seq_len(10)) {
    training <- CTM(text_DTM_trimmed[folding != chain,], k = k,
                    control = control_CTM_VEM)
    testing <- CTM(text_DTM_trimmed[folding == chain,], model = training,
                   control = control_CTM_VEM)
    perp_by_row <- rbind(perp_by_row, perplexity(testing))
  }
  perp_by_col <- cbind(perp_by_col, perp_by_row)
}

 #wow this is indeed taking forever
```

```{r}
# Plot perplexity
transpose <- t(perp_by_col)
matplot(transpose, type = "l", col = rainbow(9), lty = 2, lwd = 2, ylab = "Perplexity", xlab = "K", main = "CTM-10-fold cross validation", xaxt="n")
axis(1, at=1:10, labels = c("k=2", "k=3", "k=4", "k=5", "k=6", "k=7", "k=8", "k=9", "k=10", "k=15"), cex=0.5)

perp_by_col_mean <- colMeans(perp_by_col)

lines(perp_by_col_mean, col = "black", lwd = 4, lty = 1)
led <- c("fold=2", "fold=3", "fold=4", "fold=5", "fold=6", "fold=7", "fold=8", "fold=9", "fold=10", "Average")
legend("topright", led, lwd = 2, lty = 2, col = c(rainbow(9), 'black'), cex = 0.65)

abline(v = 4, col = "gray60", lty = 2)
```

```{r}
{plot(perp_by_col_mean, pch = 20, ylab = 'Perplexity', xlab = "K", main = "CTM-10-fold cross validation", 
      xaxt = "n") 
  axis(1, at = 1:10, labels = c("k=2","k=3","k=4","k=5","k=6","k=7","k=8","k=9","k=10","k=15"), cex = 0.5)
  lines(perp_by_col_mean, lwd = 1, lty = 2, col = "red")}
```
Alright this is  10 topics

*Correlated Topic Model*
Running 10 correlated topic model
```{r}
control_CTM_VEM1 <- list(
  estimate.beta = TRUE, verbose=0, prefix=tempfile(),save=0,keep=0,
  seed=1421313709,nstart=1L,best=TRUE,
  var=list(iter.max=500,tol=10^-6),
  em=list(iter.max=1000,tol=10^-4),
  cg=list(iter.max=500,tol=10^5)
)

################## K = 9 ##################

CTM3 <- CTM(text_DTM_trimmed, k = 10, control = control_CTM_VEM1)
CTM3
```

*Examining the Topic Output*
```{r}

## Table the documents by topics
main_topic_table3 <- as.data.frame(table(topics(CTM3)))
colnames(main_topic_table3) <- c("Topic", "Frequency")
print(main_topic_table3)


#what are the top 10 words for the 3 topics
terms(CTM3,10)
```
```{r}
# Using this: https://www.tidytextmining.com/topicmodeling.html
# Use tidyverse to look at the CTM results a bit more more

tidy_topics <- tidy(CTM3, matrix = "beta")

top_terms <- tidy_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta) 

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_fill_brewer(palette = "BuPu") +
  scale_y_reordered() +
  theme_bw()
```

### 2020-2021 Posts
```{r}
#random select 100 rows cause it is too much
posts2020 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2020posts.csv", na.string = "")
posts2021 <- fread("/Volumes/Lan's 4TB Drive/Big Data Sets/BLMReddit/reddit2021posts.csv", na.string = "")


posts2021 <- rbind(posts2020, posts2021)
ctmdat2 <- posts2021
```

```{r}
#Build CMT
corpus2 <- Corpus(VectorSource(ctmdat2$title))

#text manipulation
text_DTM2 <- DocumentTermMatrix(corpus2, control = list(stemming = TRUE, stopwords = TRUE, minWordLength = 3, 
                                                      removeNumbers = TRUE, removePunctuation = TRUE))
text_DTM2

term_tfidf2 <- tapply(text_DTM2$v / row_sums(text_DTM2)[text_DTM2$i], text_DTM2$j, mean) * log2(nDocs(text_DTM2)/col_sums(text_DTM2 > 0))
summary(term_tfidf2)

plot(density(term_tfidf2))
                            
```
Rule of thumb is to use the median from the graph for alpha. Here it's around 0.7.
We will cut down the total number of terms by setting alpha so that we don't have too many common terms no unique ones. 
```{r}
alpha2 <- 0.91 #is the median
text_DTM_trimmed2 <- text_DTM2[row_sums(text_DTM2) >0, term_tfidf >= alpha2]
dim(text_DTM_trimmed2)
```

*10-fold Cross Validation*

We set up the 10 fold cross validation, selecting 10% of the data for each of the 10 folds

```{r}
# Cross validation

control_CTM_VEM <- list(
  estimate.beta = TRUE, verbose = 0, prefix = tempfile(), save = 0, keep = 0,
  seed = as.integer(Sys.time()), nstart=1L, best = TRUE,
  var = list(iter.max=100, tol=10^-6),
  em = list(iter.max=500, tol=10^-4),
  cg = list(iter.max=100, tol=10^5)
)

# use 10-fold CV to determine k!
# randomly divide the data into 10 folds.

set.seed(123)
topics <- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15) ##set k equals to 2 3 4 5 6 7 8 9 10 15.
seed <- 2
D <- length(ctmdat2$title) 
folding <- sample(rep(seq_len(10), ceiling(D))[seq_len(D)])
table(folding)
```

*Learning Notes*
We train and test using the 10-fold cross validation that we just set up. Then plot by perplexity to see where the elbow is to select the correct number of topics k

Note: This can take a bit of time.

```{r}


#the code originally did not work because there were columns that had all 0 responses
#so we had to remove them
raw.sum2 = apply(text_DTM_trimmed2, 1, FUN = sum)
text_DTM_trimmed2 = text_DTM_trimmed2[raw.sum2!= 0,]


## write a loop to automatically output the perplexity
perp_by_col <- vector()
for (k in topics) {
  perp_by_row <- vector()
  for (chain in seq_len(10)) {
    training <- CTM(text_DTM_trimmed2[folding != chain,], k = k,
                    control = control_CTM_VEM)
    testing <- CTM(text_DTM_trimmed2[folding == chain,], model = training,
                   control = control_CTM_VEM)
    perp_by_row <- rbind(perp_by_row, perplexity(testing))
  }
  perp_by_col <- cbind(perp_by_col, perp_by_row)
}

```

```{r}
# Plot perplexity
transpose <- t(perp_by_col)
matplot(transpose, type = "l", col = rainbow(9), lty = 2, lwd = 2, ylab = "Perplexity", xlab = "K", main = "CTM-10-fold cross validation", xaxt="n")
axis(1, at=1:10, labels = c("k=2", "k=3", "k=4", "k=5", "k=6", "k=7", "k=8", "k=9", "k=10", "k=15"), cex=0.5)

perp_by_col_mean <- colMeans(perp_by_col)

lines(perp_by_col_mean, col = "black", lwd = 4, lty = 1)
led <- c("fold=2", "fold=3", "fold=4", "fold=5", "fold=6", "fold=7", "fold=8", "fold=9", "fold=10", "Average")
legend("topright", led, lwd = 2, lty = 2, col = c(rainbow(9), 'black'), cex = 0.65)

abline(v = 4, col = "gray60", lty = 2)
```

```{r}
{plot(perp_by_col_mean, pch = 20, ylab = 'Perplexity', xlab = "K", main = "CTM-10-fold cross validation", 
      xaxt = "n") 
  axis(1, at = 1:10, labels = c("k=2","k=3","k=4","k=5","k=6","k=7","k=8","k=9","k=10","k=15"), cex = 0.5)
  lines(perp_by_col_mean, lwd = 1, lty = 2, col = "red")}
```
Hmm seems like 7 topics here

*Correlated Topic Model*
Running 7 correlated topic model
```{r}
control_CTM_VEM1 <- list(
  estimate.beta = TRUE, verbose=0, prefix=tempfile(),save=0,keep=0,
  seed=1421313709,nstart=1L,best=TRUE,
  var=list(iter.max=500,tol=10^-6),
  em=list(iter.max=1000,tol=10^-4),
  cg=list(iter.max=500,tol=10^5)
)

################## K = 9 ##################

CTM4 <- CTM(text_DTM_trimmed2, k = 7, control = control_CTM_VEM1)
CTM4
```

*Examining the Topic Output*
```{r}

## Table the documents by topics
main_topic_table3 <- as.data.frame(table(topics(CTM4)))
colnames(main_topic_table3) <- c("Topic", "Frequency")
print(main_topic_table3)


#what are the top 10 words for the 3 topics
terms(CTM4,7)
```
```{r}
# Using this: https://www.tidytextmining.com/topicmodeling.html
# Use tidyverse to look at the CTM results a bit more more

tidy_topics <- tidy(CTM4, matrix = "beta")

top_terms <- tidy_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta) 

top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_fill_brewer(palette = "BuPu") +
  scale_y_reordered() +
  theme_bw()
```


